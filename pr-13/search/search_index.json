{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to QueryMT","text":"<p>QueryMT is a versatile Rust library designed to provide a unified and extensible interface for interacting with various Large Language Models (LLMs). Whether you're working with commercial APIs like OpenAI and Anthropic, or self-hosted models, QueryMT aims to simplify development by abstracting away provider-specific details.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<p>QueryMT offers a comprehensive suite of features for LLM interactions:</p> <ul> <li>Chat Interactions: Engage in conversational AI, similar to platforms like ChatGPT. QueryMT supports multi-turn dialogues, system prompts, and user/assistant roles.</li> <li>Text Completion: Generate text based on prompts, suitable for tasks like summarization, translation, or creative writing.</li> <li>Embeddings Generation: Convert text into numerical vector representations, crucial for semantic search, clustering, and other machine learning tasks.</li> <li>Tool Usage &amp; Function Calling: Enable LLMs to interact with external systems and data sources by defining and calling functions based on the conversation context.</li> <li>Extensible Plugin System: Easily add support for new LLM providers or custom logic through native shared libraries or sandboxed WebAssembly (WASM) modules via Extism.</li> <li>Prompt Chaining: Orchestrate complex workflows by linking multiple LLM calls, potentially across different providers.</li> <li>Response Evaluation: Compare and score responses from multiple LLM providers in parallel to select the best result based on custom criteria.</li> <li>MCP Integration: Connect with Model Context Protocol (MCP) servers to leverage external tools and services.</li> </ul>"},{"location":"#key-benefits","title":"Key Benefits","text":"<ul> <li>Unified Interface: Write code once and interact with multiple LLM backends using a consistent API.</li> <li>Flexibility: Choose from a variety of LLM providers, or implement your own.</li> <li>Extensibility: The plugin architecture allows for easy expansion and customization.</li> <li>Sandboxing (WASM): Safely run untrusted LLM provider logic using Extism-based WASM plugins.</li> <li>Robustness: Features like response validation and error handling improve the reliability of LLM applications.</li> <li>Developer-Friendly: Designed with Rust's strengths in performance and safety, offering a fluent builder pattern for configuration.</li> </ul> <p>This documentation will guide you through the architecture, core concepts, and various features of QueryMT, helping you leverage its power for your LLM-based projects.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>QueryMT is designed with a modular and extensible architecture to provide a flexible foundation for interacting with Large Language Models. Understanding its key components will help you leverage the library effectively.</p>"},{"location":"architecture/#core-abstractions","title":"Core Abstractions","text":"<p>At the heart of QueryMT are several key traits that define the contract for LLM interactions:</p> <ul> <li> <p><code>querymt::LLMProvider</code>: This is the central trait that all LLM provider implementations must conform to. It combines capabilities for chat, text completion, and embeddings into a single, unified interface by extending <code>querymt::chat::BasicChatProvider</code>, <code>querymt::chat::ToolChatProvider</code>, <code>querymt::completion::CompletionProvider</code>, and <code>querymt::embedding::EmbeddingProvider</code>. It also includes methods for listing available tools (<code>tools()</code>) and calling them (<code>call_tool()</code>).</p> <ul> <li>Source: <code>crates/querymt/src/lib.rs</code></li> </ul> </li> <li> <p><code>querymt::HTTPLLMProvider</code>: A specialized trait for LLM providers that are accessed over HTTP. It defines methods for constructing HTTP requests and parsing HTTP responses for chat, completion, and embedding operations by extending the <code>http</code> sub-traits (e.g., <code>querymt::chat::http::HTTPChatProvider</code>).</p> <ul> <li>Source: <code>crates/querymt/src/lib.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#provider-adapters","title":"Provider Adapters","text":"<ul> <li><code>querymt::adapters::LLMProviderFromHTTP</code>: This struct acts as an adapter, allowing an <code>HTTPLLMProvider</code> (which handles the raw HTTP logic) to be used as a full-fledged <code>LLMProvider</code>. It takes care of calling the outbound HTTP mechanism (<code>fn@querymt::outbound::call_outbound</code>) and then parsing the response using the <code>HTTPLLMProvider</code> implementation.<ul> <li>Source: <code>crates/querymt/src/adapters.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#building-and-configuring-providers","title":"Building and Configuring Providers","text":"<ul> <li><code>querymt::builder::LLMBuilder</code>: QueryMT provides a fluent builder pattern for configuring and instantiating <code>LLMProvider</code> instances. This builder allows you to set various options such as the model, API keys, temperature, custom parameters, and register tools. It uses a <code>querymt::plugin::host::PluginRegistry</code> to find the appropriate factory for the selected provider.<ul> <li>Source: <code>crates/querymt/src/builder.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#tool-and-function-calling","title":"Tool and Function Calling","text":"<p>QueryMT has robust support for LLM tool usage (often called function calling):</p> <ul> <li><code>querymt::chat::Tool</code> / <code>querymt::chat::FunctionTool</code>: These structs define the schema of tools that an LLM can use.</li> <li><code>querymt::tool_decorator::CallFunctionTool</code>: A trait that your host-side functions must implement to be callable by the LLM. It includes a method to describe the tool (<code>descriptor()</code>) and a method to execute it (<code>call()</code>).</li> <li><code>querymt::tool_decorator::ToolEnabledProvider</code>: A decorator that wraps an existing <code>LLMProvider</code> and injects tool-calling capabilities. It manages a registry of <code>CallFunctionTool</code> implementations and handles the interaction logic when an LLM decides to call a tool.<ul> <li>Source: <code>crates/querymt/src/tool_decorator.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#plugin-system","title":"Plugin System","text":"<p>A core strength of QueryMT is its plugin system, enabling easy addition of new LLM providers. The system has been unified to support different plugin types through a single registry.</p> <ul> <li><code>querymt::plugin::LLMProviderFactory</code>: A trait that plugin authors implement. Its primary role is to create an <code>LLMProvider</code> instance from a given configuration. It also provides metadata like the plugin name and configuration schema.</li> <li> <p><code>querymt::plugin::http::HTTPLLMProviderFactory</code>: A specialized factory for plugins that expose an <code>HTTPLLMProvider</code>.</p> </li> <li> <p><code>querymt::plugin::host::PluginRegistry</code>: A central registry that discovers, loads, and manages <code>LLMProviderFactory</code> instances from a configuration file. It uses different loaders for different plugin types.</p> <ul> <li><code>querymt::plugin::host::PluginLoader</code>: A trait for systems that can load a specific <code>querymt::plugin::host::PluginType</code>. QueryMT provides implementations for:<ul> <li>Native Plugins: (<code>querymt::plugin::host::native::NativeLoader</code>) Loads plugins from shared libraries (<code>.so</code>, <code>.dll</code>, <code>.dylib</code>).</li> <li>WASM Plugins via Extism: (<code>querymt::plugin::extism_impl::host::ExtismLoader</code>) Loads plugins compiled to WebAssembly and executed via Extism, offering sandboxing and portability.</li> </ul> </li> <li>Sources: <code>crates/querymt/src/plugin/host/mod.rs</code>, <code>crates/querymt/src/plugin/host/native.rs</code>, <code>crates/querymt/src/plugin/extism_impl/host/loader.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#outbound-http-communication","title":"Outbound HTTP Communication","text":"<ul> <li>The <code>outbound.rs</code> module provides a common function (<code>fn@querymt::outbound::call_outbound</code>) for making HTTP requests. This is used by HTTP-based providers and adapters. It's designed to work in both native environments (using <code>reqwest</code>) and potentially WASM environments.<ul> <li>Source: <code>crates/querymt/src/outbound.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#error-handling","title":"Error Handling","text":"<ul> <li>QueryMT defines a comprehensive <code>querymt::error::LLMError</code> to represent various issues that can occur, such as HTTP errors, authentication problems, provider-specific errors, and plugin issues.<ul> <li>Source: <code>crates/querymt/src/error.rs</code></li> </ul> </li> </ul>"},{"location":"architecture/#high-level-flow","title":"High-Level Flow","text":"<ol> <li>An application initializes a <code>querymt::plugin::host::PluginRegistry</code> from a configuration file (e.g., <code>plugins.toml</code>).</li> <li>The application registers the desired loaders (e.g., <code>NativeLoader</code>, <code>ExtismLoader</code>) with the registry.</li> <li>The registry calls <code>load_all_plugins()</code>, which iterates through the configured providers. For each provider, it determines its type (e.g., local Wasm, OCI image, native library) and uses the appropriate <code>PluginLoader</code> to load it and create an <code>LLMProviderFactory</code>.</li> <li>The application uses <code>querymt::builder::LLMBuilder</code> to configure a desired LLM provider by name (e.g., \"openai\", \"custom-plugin\").</li> <li>The builder's <code>build()</code> method is called, passing a reference to the <code>PluginRegistry</code>.</li> <li>The builder looks up the <code>LLMProviderFactory</code> from the registry using the provider name.</li> <li>The factory's <code>from_config()</code> method is called, which instantiates an <code>LLMProvider</code> (possibly an <code>HTTPLLMProvider</code> wrapped by <code>LLMProviderFromHTTP</code>).</li> <li>If tools were added via <code>add_tool()</code>, the base provider is wrapped in a <code>querymt::tool_decorator::ToolEnabledProvider</code>.</li> <li>If a validator is set, the provider is further wrapped in a <code>querymt::validated_llm::ValidatedLLM</code>.</li> <li>The application can then use the resulting <code>Box&lt;dyn LLMProvider&gt;</code> instance to perform chat, completion, or embedding operations.</li> </ol> <p>This layered and decoupled design makes QueryMT adaptable and easy to extend.</p>"},{"location":"mcp/","title":"MCP Integration","text":"<p>QueryMT includes support for integrating with Model Context Protocol (MCP) servers, leveraging the <code>rmcp</code> crate. In the context of QueryMT, MCP servers typically act as providers of tools or services that Large Language Models (LLMs) can interact with. This allows you to define complex functionalities or access external systems through a standardized MCP interface, and then make these capabilities available to your LLMs as tools.</p> <p>Key components for MCP integration are found in the <code>crates/querymt/src/mcp/</code> directory.</p>"},{"location":"mcp/#core-concepts","title":"Core Concepts","text":"<ul> <li>MCP Server: An external process or service implementing the MCP protocol (as defined by <code>rmcp</code>). It exposes a set of \"tools\" (in the MCP sense, which are similar to functions) that can be invoked.</li> <li><code>querymt::mcp::config::McpServerConfig</code>: A configuration struct in QueryMT that specifies how to connect to an MCP server.<ul> <li><code>name</code>: A unique name for this MCP server configuration.</li> <li><code>transport</code>: An <code>enum@querymt::mcp::config::McpServerTransportConfig</code> that defines the communication protocol and parameters. Supported transports include:<ul> <li><code>Http</code>: Connects to an MCP server over standard HTTP. Requires a <code>url</code> and optional <code>token</code>.</li> <li><code>Sse</code> (Server-Sent Events): Connects to an MCP server over HTTP using SSE. Requires a <code>url</code> and optional <code>token</code>.</li> <li><code>Stdio</code>: Spawns an MCP server as a child process and communicates with it over <code>stdin</code>/<code>stdout</code>. Requires the <code>command</code> to execute, optional <code>args</code>, and <code>envs</code>.</li> </ul> </li> </ul> </li> <li><code>rmcp::RoleClient</code>: The <code>rmcp</code> client used by QueryMT to communicate with an MCP server.</li> <li><code>querymt::mcp::adapter::McpToolAdapter</code>: A crucial adapter that bridges the gap between an MCP tool (<code>rmcp::model::Tool</code>) and QueryMT's tool system (<code>struct@querymt::chat::Tool</code> and <code>trait@querymt::tool_decorator::CallFunctionTool</code>).<ul> <li>It converts the MCP tool's input schema into a <code>serde_json::Value</code> that QueryMT's LLMs can understand.</li> <li>It implements <code>CallFunctionTool</code>, so an MCP tool can be registered with <code>LLMBuilder</code> just like any other native Rust tool. When the LLM decides to call this tool, the <code>McpToolAdapter</code> forwards the call to the actual MCP server via the <code>rmcp</code> client.</li> </ul> </li> </ul>"},{"location":"mcp/#workflow","title":"Workflow","text":"<ol> <li> <p>Configure MCP Servers:</p> <ul> <li>You define your MCP server connections in a configuration file (e.g., <code>mcp_config.toml</code>) that QueryMT can load using <code>mcp::config::Config::load()</code>. This file lists each MCP server and its transport details.</li> </ul> <pre><code># Example mcp_config.toml\n[[mcp]]\nname = \"my_calculator_service\"\nprotocol = \"stdio\" # or \"sse\" or \"http\"\ncommand = \"/path/to/mcp_calculator_server_binary\"\n# args = [\"--port\", \"8080\"] # if needed\n\n[[mcp]]\nname = \"external_data_api\"\nprotocol = \"sse\"\nurl = \"https://api.example.com/mcp_endpoint\"\ntoken = \"some-secret-token\"\n</code></pre> </li> <li> <p>Start MCP Clients:</p> <ul> <li>Your application uses <code>mcp::config::Config::create_mcp_clients()</code> to establish connections to all configured MCP servers. This returns a map of server names to <code>RunningService&lt;RoleClient, Box&lt;dyn DynService&lt;RoleClient&gt;&gt;&gt;</code> instances.</li> </ul> </li> <li> <p>Discover Tools from MCP Server:</p> <ul> <li>Once connected, you can query an MCP server for the list of tools it provides. The <code>rmcp</code> client would typically have a method like <code>list_tools()</code>. Each tool returned will be an <code>rmcp::model::Tool</code>.</li> </ul> </li> <li> <p>Adapt MCP Tools for QueryMT:</p> <ul> <li>For each <code>rmcp::model::Tool</code> you want to make available to your LLMs:<ul> <li>Create an <code>McpToolAdapter</code> instance using <code>McpToolAdapter::try_new(mcp_tool, server_sink)</code>, where <code>server_sink</code> is the <code>ServerSink</code> from the <code>RunningService</code> for that MCP server. This adapter converts the MCP tool's schema and handles the call forwarding.</li> </ul> </li> </ul> </li> <li> <p>Register Adapted Tools with <code>LLMBuilder</code>:</p> <ul> <li>Add the <code>McpToolAdapter</code> instances to your <code>LLMBuilder</code> using the <code>add_tool()</code> method.</li> </ul> <pre><code>// Conceptual Code\nuse querymt::builder::LLMBuilder;\nuse querymt::mcp::{config::Config as McpConfig, adapter::McpToolAdapter};\n// ... other imports ...\n\nlet mcp_config = McpConfig::load(\"mcp_config.toml\").await?;\nlet mcp_clients = mcp_config.create_mcp_clients().await?;\n\nlet calculator_client_service = mcp_clients.get(\"my_calculator_service\").unwrap();\nlet mcp_calc_tool_description = calculator_client_service.client().list_tools().await?.into_iter().find(|t| t.name == \"add\").unwrap();\n\nlet adapted_calc_tool = McpToolAdapter::try_new(\n    mcp_calc_tool_description,\n    calculator_client_service.client().sink().clone()\n)?;\n\nlet llm = LLMBuilder::new()\n    .provider(\"some_provider\")\n    // ... other configs ...\n    .add_tool(adapted_calc_tool) // Register the MCP tool adapter\n    .build(&amp;provider_registry)?;\n</code></pre> </li> <li> <p>LLM Interaction:</p> <ul> <li>When the LLM (configured with the adapted MCP tools) decides to use one of these tools:<ul> <li>QueryMT's <code>ToolEnabledProvider</code> will invoke the <code>call()</code> method on the corresponding <code>McpToolAdapter</code>.</li> <li>The adapter will then use its <code>ServerSink</code> to send a <code>CallToolRequestParam</code> to the target MCP server.</li> <li>The MCP server executes its internal logic for that tool and returns a result.</li> <li>The <code>McpToolAdapter</code> receives this result and passes it back (as a JSON string) into the QueryMT tool-calling flow.</li> <li>The LLM receives this result and continues the conversation.</li> </ul> </li> </ul> </li> </ol>"},{"location":"mcp/#benefits-of-mcp-integration","title":"Benefits of MCP Integration","text":"<ul> <li>Decoupling: Keep complex tool logic or integrations with external systems separate from your main LLM application code, managed within dedicated MCP servers.</li> <li>Standardization: Use the MCP protocol as a standard way for LLMs to discover and invoke external capabilities.</li> <li>Reusability: MCP servers and their tools can potentially be reused across multiple LLM applications or by other systems.</li> <li>Language Independence (for MCP servers): MCP servers can be written in any language, as long as they implement the MCP protocol.</li> </ul> <p>By integrating with MCP servers, QueryMT allows LLMs to leverage a broader ecosystem of tools and services in a structured and maintainable way.</p>"},{"location":"cli/","title":"QueryMT Command-Line Interface (qmt)","text":"<p>The QueryMT Command-Line Interface (<code>qmt</code>) is a powerful and versatile tool for interacting with Large Language Models directly from your terminal. It leverages the full power of the QueryMT library, providing a unified interface for chat, text embedding, tool usage, and configuration management across various LLM providers.</p>"},{"location":"cli/#key-features","title":"Key Features","text":"<ul> <li>Interactive Chat: A REPL-style interface for conversational AI.</li> <li>Piped &amp; Single-Shot Commands: Seamlessly integrate <code>qmt</code> into your shell scripts and workflows.</li> <li>Unified Provider Management: Interact with any provider configured in your <code>providers.toml</code>, from OpenAI and Anthropic to local models via Ollama.</li> <li>Text Embeddings: Generate vector embeddings for text, with support for multiple documents and custom dimensions.</li> <li>Secure Credential Storage: Manage API keys and other secrets securely.</li> <li>Tool &amp; Function Calling: Connect LLMs to external tools, including those exposed via the Model Context Protocol (MCP).</li> </ul>"},{"location":"cli/#configuration","title":"Configuration","text":"<p><code>qmt</code> relies on two primary configuration sources, typically located in <code>~/.qmt/</code>:</p> <ol> <li> <p>Provider Configuration (<code>~/.qmt/providers.toml</code>): This file defines all the available LLM provider plugins. You can specify the path to local Wasm or native plugins, or reference them from an OCI registry. This can be overridden with the <code>--provider-config</code> flag.</p> <ul> <li>For more details, see Plugin Configuration.</li> </ul> </li> <li> <p>Secret Storage (<code>~/.qmt/secrets.json</code>): This file stores sensitive information like API keys and the default provider setting. It is automatically created and managed by the <code>qmt</code> CLI and should not be edited manually.</p> </li> </ol>"},{"location":"cli/#core-commands","title":"Core Commands","text":""},{"location":"cli/#chatting-with-llms","title":"Chatting with LLMs","text":"<p><code>qmt</code> offers several ways to chat with your configured models.</p>"},{"location":"cli/#interactive-mode-repl","title":"Interactive Mode (REPL)","text":"<p>For a conversational experience, simply run <code>qmt</code> without any prompt.</p> <pre><code>$ qmt\nqmt - Interactive Chat\nProvider: openai\nType 'exit' to quit\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n:: What is the QueryMT project?\nThinking...\n&gt; Assistant: QueryMT is a versatile Rust library designed to provide a unified and extensible interface for interacting with various Large Language Models (LLMs).\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n:: exit\n\ud83d\udc4b Goodbye!\n</code></pre>"},{"location":"cli/#single-shot-prompt","title":"Single-Shot Prompt","text":"<p>Pass a prompt directly as an argument for a quick question and answer.</p> <pre><code>$ qmt \"What is the capital of France?\"\n\n&gt; Assistant: The capital of France is Paris.\n</code></pre>"},{"location":"cli/#piped-input","title":"Piped Input","text":"<p>Use standard shell pipes to provide context to the LLM. This is ideal for summarizing files, analyzing logs, or processing text and image content.</p> <pre><code># Summarize a text file\n$ cat report.txt | qmt \"Please provide a 3-sentence summary of the following text.\"\n\n# Analyze a log file\n$ cat server.log | qmt \"Are there any 'ERROR' level messages in this log?\"\n\n# Analyze an image's content (requires a provider that supports vision)\n$ cat my_image.jpg | qmt \"Describe what is happening in this image.\"\n</code></pre>"},{"location":"cli/#generating-embeddings","title":"Generating Embeddings","text":"<p>The <code>embed</code> subcommand generates vector embeddings. It can read from an argument or from stdin, and it outputs the results as a JSON array.</p> <pre><code># Embed a single string\n$ qmt embed \"This is a test sentence.\"\n[[\n  -0.0189,\n  0.0234,\n  ...\n  -0.0056\n]]\n\n# Embed a file's content\n$ cat document.txt | qmt embed\n</code></pre> <p>You can also embed multiple documents at once by specifying a separator.</p> <pre><code># Embed two documents separated by \"---\"\n$ echo \"First document.---Second document.\" | qmt embed --separator \"---\"\n[\n  [\n    -0.01, ...\n  ],\n  [\n    0.02, ...\n  ]\n]\n</code></pre>"},{"location":"cli/#managing-configuration-secrets","title":"Managing Configuration &amp; Secrets","text":"<p><code>qmt</code> includes commands to manage your settings without manually editing files.</p>"},{"location":"cli/#setting-the-default-provider","title":"Setting the Default Provider","text":"<p>Set your most-used provider and model combination as the default.</p> <pre><code># Set the default provider to OpenAI's gpt-4-turbo model\n$ qmt default openai:gpt-4-turbo\n\n# Check the current default\n$ qmt default\nDefault provider: openai:gpt-4-turbo\n</code></pre>"},{"location":"cli/#storing-api-keys","title":"Storing API Keys","text":"<p>Securely store API keys. The <code>key</code> should match the <code>api_key_name</code> defined by the provider plugin (e.g., <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>).</p> <pre><code># Store your OpenAI API key\n$ qmt set OPENAI_API_KEY \"sk-...\"\n\u2713 Secret 'OPENAI_API_KEY' has been set.\n\n# Retrieve a stored key (for verification)\n$ qmt get OPENAI_API_KEY\nOPENAI_API_KEY: sk-...\n\n# Delete a key\n$ qmt delete OPENAI_API_KEY\n\u2713 Secret 'OPENAI_API_KEY' has been deleted.\n</code></pre>"},{"location":"cli/#discovering-providers-models","title":"Discovering Providers &amp; Models","text":"<p>List the providers and models that <code>qmt</code> has loaded from your configuration.</p> <pre><code># List all configured provider plugins\n$ qmt providers\n- openai\n- anthropic\n- ollama-plugin\n\n# List all available models for each provider\n$ qmt models\nopenai:\n  - openai:gpt-4-turbo\n  - openai:gpt-3.5-turbo\nanthropic:\n  - anthropic:claude-3-opus-20240229\n  - anthropic:claude-3-sonnet-20240229\nollama-plugin:\n  - ollama-plugin:llama3\n  - ollama-plugin:mistral\n</code></pre>"},{"location":"cli/#examples-advanced-usage","title":"Examples &amp; Advanced Usage","text":"<p>Combine flags to customize LLM interactions.</p>"},{"location":"cli/#specifying-a-provider-and-model","title":"Specifying a Provider and Model","text":"<p>Use the <code>--provider</code> (<code>-p</code>) and <code>--model</code> flags, or combine them in the format <code>provider:model</code>.</p> <pre><code># Use the anthropic provider with the claude-3-sonnet model\n$ qmt -p anthropic --model claude-3-sonnet-20240229 \"Explain the concept of emergence.\"\n\n# A more concise way to do the same\n$ qmt -p anthropic:claude-3-sonnet-20240229 \"Explain the concept of emergence.\"\n</code></pre>"},{"location":"cli/#using-a-local-model-via-a-proxylocal-server","title":"Using a Local Model via a Proxy/Local Server","text":"<p>Override the <code>--base-url</code> to point to a local service like Ollama.</p> <pre><code># Assuming you have an 'ollama' provider configured that works with OpenAI's API format\n$ qmt -p ollama:llama3 --base-url http://localhost:11434 \"Who are you?\"\n</code></pre>"},{"location":"cli/#setting-a-system-prompt","title":"Setting a System Prompt","text":"<p>Guide the model's behavior with a system prompt using <code>--system</code> (<code>-s</code>).</p> <pre><code>$ qmt -s \"You are a helpful assistant that only speaks in rhyme.\" \"What is a computer?\"\n</code></pre>"},{"location":"cli/#controlling-generation-parameters","title":"Controlling Generation Parameters","text":"<p>Adjust temperature, max tokens, and other parameters for more creative or controlled output.</p> <pre><code>$ qmt --temperature 1.2 --max-tokens 100 \"Write a short, creative opening line for a fantasy novel.\"\n</code></pre>"},{"location":"cli/#passing-custom-provider-specific-options","title":"Passing Custom Provider-Specific Options","text":"<p>Use the <code>--options</code> (<code>-o</code>) flag to pass arbitrary key-value pairs that a specific plugin might support. Values are parsed as JSON if possible, otherwise as strings.</p> <pre><code>$ qmt -p some-custom-provider -o vision_mode=true -o response_format='{\"type\": \"json_object\"}' \"Analyze this image\"\n</code></pre>"},{"location":"cli/#embedding-with-specific-dimensions","title":"Embedding with Specific Dimensions","text":"<p>Some embedding models support custom dimensions.</p> <pre><code>$ qmt embed \"This is a test\" -p openai:text-embedding-3-small --dimensions 512\n</code></pre>"},{"location":"cli/#using-tools-via-mcp","title":"Using Tools via MCP","text":"<p>Create a file named <code>mcp-config.toml</code>. This file tells QueryMT how to launch and communicate with the tool server.</p> <pre><code># mcp-config.toml\n[[mcp]]\nname = \"mem_server\"\nprotocol = \"stdio\"\ncommand = \"npx\"\nargs = [\"-y\", \"@modelcontextprotocol/server-memory\"]\n</code></pre> <p>When you ask a question that requires a tool, the LLM will request to use it, and <code>qmt</code> will handle the communication.</p> <pre><code># Ask the LLM to use the 'set' tool from our stdio server\n$ qmt --provider openai:gpt-4o-mini --mcp-config ./mcp-config.toml\nqmt - Interactive Chat\nProvider: openai\nType 'exit' to quit\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n:: create two entities: qmt and awesome and connect qmt with an isa relation to awesome\n\u250c\u2500 create_entities\n\u2514\u2500 calling...\n\u250c\u2500 create_entities\n\u2514\u2500 generated\n&gt; Assistant: The entities \"qmt\" and \"awesome\" have been successfully created, and the relation \"isa\" has been established from \"qmt\" to \"awesome.\" If you need any further modifications or additional actions, just let me know!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n:: what is qmt?\n\u250c\u2500 open_nodes\n\u2514\u2500 generated\n&gt; Assistant: The entity \"qmt\" is of type \"type1\" and currently has no associated observations or relations other than the relation to \"awesome\" established earlier. If you need more information or to add observations, feel free to ask!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n::\n\ud83d\udc4b Goodbye!\n</code></pre>"},{"location":"cli/#command-line-reference","title":"Command-Line Reference","text":""},{"location":"cli/#global-options","title":"Global Options","text":"<p>These options can be used with the main chat command.</p> Flag Alias Description <code>--provider &lt;NAME&gt;</code> <code>-p</code> The name of the provider to use (e.g., <code>openai</code>, <code>anthropic:claude-3</code>). <code>--model &lt;NAME&gt;</code> The specific model to use (e.g., <code>gpt-4-turbo</code>). <code>--system &lt;PROMPT&gt;</code> <code>-s</code> The system prompt to set the context for the conversation. <code>--api-key &lt;KEY&gt;</code> Directly provide an API key, overriding stored secrets or environment vars. <code>--base-url &lt;URL&gt;</code> The base URL for the LLM API, for use with proxies or local models. <code>--temperature &lt;FLOAT&gt;</code> Controls randomness (e.g., <code>0.7</code>). <code>--top-p &lt;FLOAT&gt;</code> Nucleus sampling parameter. <code>--top-k &lt;INT&gt;</code> Top-k sampling parameter. <code>--max-tokens &lt;INT&gt;</code> The maximum number of tokens to generate in the response. <code>--mcp-config &lt;PATH&gt;</code> Path to a TOML configuration file for MCP tool servers. <code>--provider-config &lt;PATH&gt;</code> Path to the provider plugins configuration file (e.g., <code>plugins.toml</code>). <code>--options &lt;KEY=VALUE&gt;</code> <code>-o</code> Set a provider-specific parameter. Can be used multiple times."},{"location":"cli/#subcommands","title":"Subcommands","text":"Command Description <code>qmt set &lt;KEY&gt; &lt;VALUE&gt;</code> Set a secret key-value pair in the secure store. <code>qmt get &lt;KEY&gt;</code> Get a secret value by its key. <code>qmt delete &lt;KEY&gt;</code> Delete a secret by its key. <code>qmt providers</code> List all available provider plugins loaded from the configuration. <code>qmt models</code> List all available models for each provider. <code>qmt default [PROVIDER]</code> Get or set the default provider (e.g., <code>qmt default openai:gpt-4</code>). <code>qmt embed [TEXT]</code> Generate embeddings for the given text or for text from stdin. <code>--separator &lt;STR&gt;</code> Document separator for embedding multiple texts from a single stream. <code>--dimensions &lt;INT&gt;</code> Request a specific number of dimensions for the embedding vector. <code>--encoding-format &lt;FMT&gt;</code> Specify the embedding encoding format (e.g., <code>float</code>, <code>base64</code>). <code>--provider &lt;NAME&gt;</code> Override the provider for this embedding task. <code>--model &lt;NAME&gt;</code> Override the model for this embedding task."},{"location":"core/","title":"Core Concepts Overview","text":"<p>QueryMT is built around a set of core concepts that enable flexible and powerful interactions with Large Language Models. This section provides an overview of these fundamental ideas. Each concept is explored in more detail in its dedicated page.</p> <ul> <li> <p>LLM Providers:     The abstraction layer for different LLM services (e.g., OpenAI, Anthropic, local models). QueryMT allows you to interact with them through a unified interface.</p> </li> <li> <p>Chat Interactions:     Engage in conversational AI. This involves sending sequences of messages (from users, assistants, or even tool calls) and receiving responses from the LLM.</p> </li> <li> <p>Text Completion:     Generate text based on a given prompt. Useful for tasks like summarization, translation, or content creation.</p> </li> <li> <p>Embeddings:     Convert text into numerical vector representations (embeddings). These are essential for semantic search, clustering, and other machine learning applications.</p> </li> <li> <p>Tools &amp; Function Calling:     Empower LLMs to interact with external systems. You can define tools (functions) that the LLM can choose to call, allowing it to access real-time information or perform actions.</p> </li> <li> <p>Building Providers (LLMBuilder):     A fluent builder pattern (<code>LLMBuilder</code>) simplifies the configuration and instantiation of LLM provider instances, allowing you to specify models, API keys, behavior parameters, and tools.</p> </li> <li> <p>Chaining Prompts:     Orchestrate complex workflows by linking multiple LLM calls in sequence. The output of one step can be used as input for the next, enabling sophisticated reasoning and task decomposition.</p> </li> </ul> <p>Understanding these concepts will provide a solid foundation for using QueryMT effectively in your projects.</p>"},{"location":"core/builder/","title":"Building Providers (LLMBuilder)","text":"<p>QueryMT uses a fluent builder pattern, embodied by the <code>querymt::builder::LLMBuilder</code>, to simplify the configuration and instantiation of <code>LLMProvider</code> instances. This approach allows you to chain configuration methods in a readable and expressive way.</p> <p>The <code>LLMBuilder</code> is defined in <code>crates/querymt/src/builder.rs</code>.</p>"},{"location":"core/builder/#core-functionality","title":"Core Functionality","text":"<p>The <code>LLMBuilder</code> allows you to set various common and provider-specific parameters before finally building the <code>LLMProvider</code>.</p>"},{"location":"core/builder/#common-configuration-options","title":"Common Configuration Options:","text":"<ul> <li><code>provider(name: String)</code>: Specifies the name of the LLM provider to use (e.g., \"openai\", \"anthropic\", \"my-custom-plugin\"). This name is used to look up the corresponding <code>LLMProviderFactory</code> from a <code>PluginRegistry</code>.</li> <li><code>api_key(key: String)</code>: Sets the API key for authentication with the provider.</li> <li><code>base_url(url: String)</code>: Sets a custom base URL, often used for self-hosted models or proxies.</li> <li><code>model(model_id: String)</code>: Specifies the model identifier to use (e.g., \"gpt-4\", \"claude-2.1\").</li> <li><code>max_tokens(tokens: u32)</code>: Sets the maximum number of tokens the LLM should generate in its response.</li> <li><code>temperature(temp: f32)</code>: Controls the randomness of the output (typically 0.0 to 1.0).</li> <li><code>system(prompt: String)</code>: Provides a system-level prompt or instructions to guide the LLM's behavior.</li> <li><code>timeout_seconds(seconds: u64)</code>: Sets a timeout for requests to the LLM provider.</li> <li><code>stream(enable: bool)</code>: Enables or disables streaming responses (if supported by the provider).</li> <li><code>top_p(p: f32)</code>, <code>top_k(k: u32)</code>: Parameters for nucleus and top-k sampling.</li> <li><code>embedding_encoding_format(format: String)</code>, <code>embedding_dimensions(dims: u32)</code>: Configuration for embedding generation.</li> <li><code>schema(schema: StructuredOutputFormat)</code>: Defines a JSON schema for structured output from the LLM.</li> <li><code>validator(func: F)</code>: Sets a custom validation function for LLM responses.</li> <li><code>validator_attempts(attempts: usize)</code>: Sets the number of retries if validation fails.</li> <li><code>add_tool(tool: T)</code>: Registers a tool (an implementation of <code>CallFunctionTool</code>) to be made available to the LLM. See Tools &amp; Function Calling.</li> <li><code>tool_choice(choice: ToolChoice)</code>: Specifies how the LLM should use tools (e.g., auto, force specific tool).</li> <li><code>enable_parallel_tool_use(enable: bool)</code>: Allows the model to call multiple tools in parallel, if supported.</li> <li><code>reasoning(enable: bool)</code>: Enables the model to output its reasoning or \"thinking\" steps.</li> <li><code>reasoning_effort(effort: ReasoningEffort)</code>: Specifies the desired level of reasoning effort (<code>Low</code>, <code>Medium</code>, <code>High</code>).</li> <li><code>reasoning_budget_tokens(tokens: u32)</code>: Allocates a token budget for the model's reasoning process.</li> <li><code>parameter(key: String, value: Value)</code>: Allows setting arbitrary provider-specific parameters.</li> </ul>"},{"location":"core/builder/#building-the-provider","title":"Building the Provider","text":"<ul> <li><code>build(self, registry: &amp;PluginRegistry) -&gt; Result&lt;Box&lt;dyn LLMProvider&gt;, LLMError&gt;</code>:     This is the final step. It takes a reference to a <code>querymt::plugin::host::PluginRegistry</code> (which knows how to create different providers).<ol> <li>It serializes the builder's configuration into a JSON <code>Value</code>.</li> <li>It retrieves the appropriate <code>LLMProviderFactory</code> from the <code>registry</code> based on the <code>provider</code> name set earlier.</li> <li>It prunes the full configuration based on the schema provided by the factory, so only relevant options are passed.</li> <li>It calls <code>factory.from_config()</code> with the pruned configuration to get a base <code>LLMProvider</code>.</li> <li>If tools were added via <code>add_tool()</code>, it wraps the base provider in a <code>querymt::tool_decorator::ToolEnabledProvider</code>.</li> <li>If a <code>validator</code> was set, it further wraps the provider in a <code>querymt::validated_llm::ValidatedLLM</code>.</li> <li>Returns the fully configured <code>LLMProvider</code> instance, boxed as a trait object.</li> </ol> </li> </ul>"},{"location":"core/builder/#example-usage-conceptual","title":"Example Usage (Conceptual)","text":"<pre><code>use querymt::builder::LLMBuilder;\nuse querymt::chat::ToolChoice;\nuse querymt::plugin::host::PluginRegistry; // Assuming you have a registry instance\n// Assume GetWeatherTool is an impl CallFunctionTool\n// use my_tools::GetWeatherTool;\n\nasync fn setup_llm_provider(registry: &amp;PluginRegistry) -&gt; Result&lt;Box&lt;dyn querymt::LLMProvider&gt;, querymt::error::LLMError&gt; {\n    let llm = LLMBuilder::new()\n        .provider(\"openai\") // Or your plugin's name\n        .model(\"gpt-4-turbo\")\n        .api_key(\"YOUR_OPENAI_API_KEY\")\n        .temperature(0.7)\n        .max_tokens(500)\n        .system(\"You are a helpful assistant.\")\n        // .add_tool(GetWeatherTool) // Example of adding a tool\n        // .tool_choice(ToolChoice::Auto)\n        .validator(|response_text| {\n            if response_text.to_lowercase().contains(\"sorry\") {\n                Err(\"Response should not be apologetic.\".to_string())\n            } else {\n                Ok(())\n            }\n        })\n        .validator_attempts(2)\n        .build(registry)?;\n\n    Ok(llm)\n}\n</code></pre> <p>The <code>LLMBuilder</code> provides a convenient and type-safe way to configure diverse LLM providers with a wide range of options, abstracting away the underlying factory and wrapping logic.</p>"},{"location":"core/chaining/","title":"Chaining Prompts","text":"<p>QueryMT allows you to orchestrate sequences of LLM interactions, known as \"prompt chains.\" This is useful for breaking down complex tasks into smaller, manageable steps, where the output of one LLM call can be used as input for subsequent calls. QueryMT provides two main mechanisms for this: <code>PromptChain</code> for single-provider chains and <code>MultiPromptChain</code> for chains involving multiple different providers.</p> <p>These are primarily defined in <code>crates/querymt/src/chain/mod.rs</code> and <code>crates/querymt/src/chain/multi.rs</code>.</p>"},{"location":"core/chaining/#promptchain-single-provider","title":"<code>PromptChain</code> (Single Provider)","text":"<p>A <code>querymt::chain::PromptChain</code> executes a sequence of steps using a single <code>LLMProvider</code> instance.</p>"},{"location":"core/chaining/#key-components","title":"Key Components:","text":"<ul> <li><code>querymt::chain::ChainStep</code>: Defines a single step in the chain.<ul> <li><code>id</code>: A unique identifier for this step. The output of this step will be stored in memory using this ID.</li> <li><code>template</code>: A prompt template string. It can contain placeholders like <code>{{variable_name}}</code>, which will be replaced by outputs from previous steps (stored in memory).</li> <li><code>mode</code>: <code>enum@querymt::chain::ChainStepMode::Chat</code> or <code>enum@querymt::chain::ChainStepMode::Completion</code>, indicating how this step should be executed.</li> <li>Optional parameters like <code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>.</li> </ul> </li> <li><code>querymt::chain::PromptChain&lt;'a&gt;</code>: Manages the sequence of <code>ChainStep</code>s.<ul> <li>It holds a reference to an <code>LLMProvider</code> and a <code>memory</code> (HashMap) to store the outputs of each step.</li> <li><code>new(llm: &amp;'a dyn LLMProvider)</code>: Creates a new chain.</li> <li><code>step(step: ChainStep)</code>: Adds a step to the chain.</li> <li><code>run()</code>: Executes all steps in sequence.<ul> <li>For each step, it applies the template using values from memory.</li> <li>Executes the step using the LLM (chat or completion).</li> <li>Stores the LLM's response text in memory, keyed by the step's <code>id</code>.</li> <li>Returns the final memory map containing all step outputs.</li> </ul> </li> </ul> </li> </ul>"},{"location":"core/chaining/#example-conceptual-promptchain","title":"Example (Conceptual <code>PromptChain</code>):","text":"<pre><code>// Assuming 'llm' is a Box&lt;dyn LLMProvider&gt;\nuse querymt::chain::{PromptChain, ChainStepBuilder, ChainStepMode};\n\nlet chain = PromptChain::new(&amp;*llm)\n    .step(\n        ChainStepBuilder::new(\"step1_idea\", \"Generate a topic for a short story.\", ChainStepMode::Chat)\n            .build()\n    )\n    .step(\n        ChainStepBuilder::new(\"step2_plot\", \"Write a brief plot for a story about: {{step1_idea}}\", ChainStepMode::Chat)\n            .max_tokens(200)\n            .build()\n    );\n\nlet results = chain.run().await?;\nprintln!(\"Generated Plot: {}\", results.get(\"step2_plot\").unwrap_or_default());\n</code></pre>"},{"location":"core/chaining/#multipromptchain-multiple-providers","title":"<code>MultiPromptChain</code> (Multiple Providers)","text":"<p>A <code>querymt::chain::MultiPromptChain</code> allows you to define steps that can be executed by different LLM providers, registered in an <code>querymt::chain::LLMRegistry</code>. It also supports complex, iterative tool-calling within each step.</p>"},{"location":"core/chaining/#key-components_1","title":"Key Components:","text":"<ul> <li><code>querymt::chain::LLMRegistry</code>: A collection (<code>HashMap</code>) that stores multiple <code>LLMProvider</code> instances, each identified by a unique string key (e.g., \"openai\", \"anthropic-haiku\").<ul> <li><code>querymt::chain::LLMRegistryBuilder</code> provides a fluent way to construct this registry.</li> </ul> </li> <li><code>querymt::chain::multi::MultiChainStep</code>: Similar to <code>ChainStep</code>, but includes:<ul> <li><code>provider_id</code>: The string key of the <code>LLMProvider</code> in the <code>LLMRegistry</code> that should execute this step.</li> <li><code>response_transform</code>: An optional function <code>Box&lt;dyn Fn(String) -&gt; String + Send + Sync&gt;</code> to transform the raw string output of the LLM before storing it in memory.</li> </ul> </li> <li><code>querymt::chain::multi::MultiChainStepBuilder</code>: Builder for <code>MultiChainStep</code>.</li> <li><code>querymt::chain::MultiPromptChain&lt;'a&gt;</code>: Manages the sequence of <code>MultiChainStep</code>s.<ul> <li>Holds a reference to an <code>LLMRegistry</code>.</li> <li>The <code>run()</code> method works similarly to <code>PromptChain</code>, but for each step, it retrieves the specified <code>LLMProvider</code> from the registry before execution.</li> <li>Tool Calling Loop: Within a single chat step, <code>MultiPromptChain</code> can handle iterative tool calls. If the LLM responds with a request to call a tool, the chain will execute the tool via the provider's <code>call_tool</code> method, append the result to the conversation history, and send it back to the LLM. This loop continues until the LLM provides a final text response without any tool calls.</li> </ul> </li> </ul>"},{"location":"core/chaining/#example-conceptual-multipromptchain","title":"Example (Conceptual <code>MultiPromptChain</code>):","text":"<pre><code>// Assuming 'registry' is an LLMRegistry with providers \"fast_model\" and \"creative_model\"\nuse querymt::chain::{MultiPromptChain, multi::{MultiChainStepBuilder, MultiChainStepMode}, LLMRegistryBuilder};\n\n// let registry = LLMRegistryBuilder::new()\n//     .register(\"fast_model\", fast_llm_provider)\n//     .register(\"creative_model\", creative_llm_provider)\n//     .build();\n\nlet chain = MultiPromptChain::new(&amp;registry)\n    .step(\n        MultiChainStepBuilder::new(MultiChainStepMode::Chat)\n            .provider_id(\"fast_model\")\n            .id(\"step1_keywords\")\n            .template(\"Extract 3 keywords from this text: 'The future of AI is exciting.'\")\n            .build()?\n    )\n    .step(\n        MultiChainStepBuilder::new(MultiChainStepMode::Completion)\n            .provider_id(\"creative_model\")\n            .id(\"step2_tagline\")\n            .template(\"Generate a catchy tagline using these keywords: {{step1_keywords}}\")\n            .max_tokens(50)\n            .response_transform(|s| s.trim().to_uppercase()) // Example transform\n            .build()?\n    );\n\nlet results = chain.run().await?;\nprintln!(\"Generated Tagline: {}\", results.get(\"step2_tagline\").unwrap_or_default());\n</code></pre> <p>Prompt chaining is a powerful technique for building more sophisticated LLM applications by decomposing tasks and leveraging the strengths of different models or configurations for different parts of a workflow.</p>"},{"location":"core/chat/","title":"Chat Interactions","text":"<p>QueryMT provides comprehensive support for chat-based interactions with Large Language Models, enabling you to build conversational AI applications.</p>"},{"location":"core/chat/#key-components","title":"Key Components","text":"<ul> <li> <p><code>querymt::chat::ChatMessage</code>: Represents a single message in a conversation. Key attributes include:</p> <ul> <li><code>role</code>: Indicates who sent the message (<code>querymt::chat::ChatRole::User</code> or <code>querymt::chat::ChatRole::Assistant</code>).</li> <li><code>message_type</code>: Specifies the nature of the content (e.g., <code>querymt::chat::MessageType::Text</code>, <code>Image</code>, <code>ToolUse</code>, <code>ToolResult</code>).</li> <li><code>content</code>: The primary text content of the message.</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> <li> <p><code>querymt::chat::ChatResponse</code>: A trait representing the LLM's response to a chat request. It provides methods to access:</p> <ul> <li><code>text()</code>: The textual content of the LLM's reply.</li> <li><code>tool_calls()</code>: A list of <code>querymt::ToolCall</code> objects if the LLM decided to use one or more tools.</li> <li><code>thinking()</code>: Optional \"thoughts\" or reasoning steps from the model, if supported and enabled.</li> <li><code>usage()</code>: Optional token usage information (<code>querymt::Usage</code>).</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> <li> <p><code>querymt::chat::BasicChatProvider</code>: A trait that LLM providers implement to support fundamental chat functionality. It has a single method:</p> <ul> <li><code>chat(&amp;self, messages: &amp;[ChatMessage])</code>: Sends a list of messages to the LLM and returns a <code>ChatResponse</code>.</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> <li> <p><code>querymt::chat::ToolChatProvider</code>: Extends <code>BasicChatProvider</code> to include support for tools (function calling). It has one primary method:</p> <ul> <li><code>chat_with_tools(&amp;self, messages: &amp;[ChatMessage], tools: Option&lt;&amp;[Tool]&gt;)</code>: Sends messages along with a list of available tools the LLM can use.</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> </ul>"},{"location":"core/chat/#how-it-works","title":"How It Works","text":"<ol> <li>Construct Messages: Your application assembles a sequence of <code>ChatMessage</code> objects representing the conversation history using the <code>querymt::chat::ChatMessageBuilder</code>. This typically starts with alternating <code>User</code> and <code>Assistant</code> messages.</li> <li>Initiate Chat: You call the <code>chat</code> or <code>chat_with_tools</code> method on an <code>LLMProvider</code> instance, passing the message history and optionally, a list of available tools.</li> <li>Provider Interaction: The <code>LLMProvider</code> (or its underlying implementation like <code>HTTPLLMProvider</code>) formats the request according to the specific LLM's API, sends it, and receives the raw response.</li> <li>Parse Response: The provider parses the raw response into an object implementing <code>ChatResponse</code>.</li> <li>Handle Response: Your application processes the <code>ChatResponse</code>:<ul> <li>If <code>text()</code> is present, it's the LLM's textual reply.</li> <li>If <code>tool_calls()</code> is present, the LLM wants to execute one or more functions. Your application needs to:<ul> <li>Execute these functions.</li> <li>Send the results back to the LLM as new <code>ChatMessage</code>s (typically with <code>MessageType::ToolResult</code>).</li> <li>Continue the chat loop.</li> </ul> </li> </ul> </li> </ol>"},{"location":"core/chat/#example-flow-conceptual","title":"Example Flow (Conceptual)","text":"<pre><code>use querymt::chat::{ChatMessage, ChatMessageBuilder, ChatResponse, MessageType, Tool, ToolCall};\nuse querymt::LLMProvider;\nuse serde_json::Value;\n\n// Assuming 'llm_provider' is an instance of Box&lt;dyn LLMProvider&gt; that has tools registered\n// and 'my_tools' is a Vec&lt;Tool&gt; describing them.\n\nasync fn handle_tool_calling_loop(\n    llm_provider: &amp;Box&lt;dyn LLMProvider&gt;,\n    initial_messages: Vec&lt;ChatMessage&gt;,\n    my_tools: &amp;[Tool],\n) -&gt; anyhow::Result&lt;String&gt; {\n    let mut messages = initial_messages;\n\n    loop {\n        let response = llm_provider.chat_with_tools(&amp;messages, Some(my_tools)).await?;\n\n        // Add the assistant's response to the history. It might contain text and/or tool calls.\n        messages.push(\n            ChatMessage::assistant()\n                .content(response.text().unwrap_or_default())\n                .tool_use(response.tool_calls().unwrap_or_default()) // This will be empty if no tools were called\n                .build(),\n        );\n\n        if let Some(tool_calls) = response.tool_calls() {\n            if tool_calls.is_empty() {\n                // No tool calls, so the text response is the final answer.\n                return Ok(response.text().unwrap_or_default());\n            }\n\n            // The model wants to call one or more tools.\n            let mut tool_results = Vec::new();\n            for call in tool_calls {\n                println!(\"LLM wants to call tool: {} with args: {}\", call.function.name, call.function.arguments);\n\n                // In a real app, you would dispatch to your tool execution logic here.\n                // The `LLMProvider` trait has a `call_tool` helper for this.\n                let args: Value = serde_json::from_str(&amp;call.function.arguments)?;\n                let result_str = llm_provider.call_tool(&amp;call.function.name, args).await?;\n\n                // Create a ToolCall struct containing the result.\n                // Note: The result is placed in the `arguments` field for transport.\n                let result_call = ToolCall {\n                    id: call.id,\n                    call_type: \"function\".to_string(),\n                    function: querymt::FunctionCall {\n                        name: call.function.name,\n                        arguments: result_str,\n                    },\n                };\n                tool_results.push(result_call);\n            }\n\n            // Add the tool results back to the conversation history.\n            messages.push(ChatMessage::user().tool_result(tool_results).build());\n            // Loop again to let the model process the tool results.\n        } else {\n            // No tool calls in the response, so the text is the final answer.\n            return Ok(response.text().unwrap_or_default());\n        }\n    }\n}\n</code></pre> <p>QueryMT's chat system is designed to be flexible, supporting simple Q&amp;A, complex multi-turn dialogues, and sophisticated interactions involving external tools. The <code>Tool</code> and <code>ToolChoice</code> mechanisms provide fine-grained control over how LLMs can utilize functions.</p>"},{"location":"core/completion/","title":"Text Completion","text":"<p>Text completion is a fundamental capability of Large Language Models where the model generates text that continues from a given input prompt. QueryMT provides a standardized way to perform text completion tasks.</p>"},{"location":"core/completion/#key-components","title":"Key Components","text":"<ul> <li> <p><code>CompletionRequest</code>: Represents a request for text completion. It includes:</p> <ul> <li><code>prompt</code>: The input text that the LLM should complete.</li> <li><code>suffix</code>: Optional text that should appear after the model's completion.</li> <li><code>max_tokens</code>: Optional limit on the number of tokens to generate.</li> <li><code>temperature</code>: Optional parameter (0.0-1.0) to control the randomness of the output. Higher values make the output more random, while lower values make it more deterministic.</li> <li>Source: <code>crates/querymt/src/completion/mod.rs</code></li> </ul> </li> <li> <p><code>CompletionResponse</code>: Represents the LLM's generated completion. It primarily contains:</p> <ul> <li><code>text</code>: The generated text string.</li> <li>It also implements the <code>ChatResponse</code> trait, meaning its <code>text()</code> method can be used to retrieve the completion, but <code>tool_calls()</code> will typically be <code>None</code> for completions.</li> <li>Source: <code>crates/querymt/src/completion/mod.rs</code></li> </ul> </li> <li> <p><code>CompletionProvider</code>: A trait that LLM providers implement to support text completion. It has a single method:</p> <ul> <li><code>complete(&amp;self, req: &amp;CompletionRequest)</code>: Sends a completion request to the LLM and returns a <code>CompletionResponse</code>.</li> <li>Source: <code>crates/querymt/src/completion/mod.rs</code></li> </ul> </li> </ul>"},{"location":"core/completion/#how-it-works","title":"How It Works","text":"<ol> <li>Create Request: Your application creates a <code>CompletionRequest</code> object, providing the prompt and any optional parameters like <code>max_tokens</code> or <code>temperature</code>. QueryMT offers a <code>CompletionRequest::builder()</code> for a more fluent way to construct these requests.</li> <li>Send Request: You call the <code>complete</code> method on an <code>LLMProvider</code> instance, passing the <code>CompletionRequest</code>.</li> <li>Provider Interaction: The <code>LLMProvider</code> (or its underlying <code>HTTPCompletionProvider</code> if it's an HTTP-based model) formats the request according to the specific LLM's API, sends it, and receives the raw response.</li> <li>Parse Response: The provider parses the raw response into a <code>CompletionResponse</code> object.</li> <li>Use Completion: Your application can then access the generated text from the <code>CompletionResponse.text</code> field.</li> </ol>"},{"location":"core/completion/#example-flow-conceptual","title":"Example Flow (Conceptual)","text":"<pre><code>// Assuming 'llm_provider' is an instance of Box&lt;dyn LLMProvider&gt;\n\nlet request = CompletionRequest::builder(\"Once upon a time, in a land far, far away,\")\n    .max_tokens(100)\n    .temperature(0.7)\n    .build();\n\nmatch llm_provider.complete(&amp;request).await {\n    Ok(response) =&gt; {\n        println!(\"Generated story: {}\", response.text);\n    }\n    Err(e) =&gt; {\n        eprintln!(\"Error during completion: {}\", e);\n    }\n}\n</code></pre> <p>Text completion is a versatile tool for tasks such as:</p> <ul> <li>Drafting emails or documents.</li> <li>Summarizing text.</li> <li>Translating languages.</li> <li>Generating code snippets.</li> <li>Creative writing and storytelling.</li> </ul> <p>QueryMT's <code>CompletionProvider</code> abstraction ensures that you can switch between different LLM backends for completion tasks without significantly changing your application code.</p>"},{"location":"core/embeddings/","title":"Embeddings","text":"<p>Embeddings are numerical representations of text (or other data types) in a high-dimensional vector space. These vectors capture the semantic meaning of the input, such that pieces of text with similar meanings will have vectors that are close together in this space. QueryMT provides a way to generate embeddings using LLMs.</p>"},{"location":"core/embeddings/#key-components","title":"Key Components","text":"<ul> <li><code>EmbeddingProvider</code>: A trait that LLM providers implement to support the generation of text embeddings. It has a single core method:<ul> <li><code>embed(&amp;self, inputs: Vec&lt;String&gt;)</code>: Takes a vector of input strings and returns a <code>Result</code> containing a vector of embedding vectors (<code>Vec&lt;Vec&lt;f32&gt;&gt;</code>), where each inner vector corresponds to an input string. Each <code>f32</code> value is a dimension of the embedding.</li> <li>Source: <code>crates/querymt/src/embedding/mod.rs</code></li> </ul> </li> </ul>"},{"location":"core/embeddings/#how-it-works","title":"How It Works","text":"<ol> <li>Prepare Inputs: Your application prepares a list of text strings for which you want to generate embeddings.</li> <li>Request Embeddings: You call the <code>embed</code> method on an <code>LLMProvider</code> instance, passing the vector of input strings.</li> <li>Provider Interaction: The <code>LLMProvider</code> (or its underlying <code>HTTPEmbeddingProvider</code> for HTTP-based models) formats the request according to the specific LLM's API, sends it, and receives the raw response containing the embedding data.</li> <li>Parse Response: The provider parses the raw response and converts it into a <code>Vec&lt;Vec&lt;f32&gt;&gt;</code>, where each inner vector is an embedding for the corresponding input string.</li> <li>Use Embeddings: Your application can then use these embedding vectors for various downstream tasks.</li> </ol>"},{"location":"core/embeddings/#example-flow-conceptual","title":"Example Flow (Conceptual)","text":"<pre><code>// Assuming 'llm_provider' is an instance of Box&lt;dyn LLMProvider&gt;\n\nlet texts_to_embed = vec![\n    \"The quick brown fox jumps over the lazy dog.\".to_string(),\n    \"A fast, agile, caramel-colored canine leaps above a sleepy hound.\".to_string(),\n    \"The weather is sunny today.\".to_string(),\n];\n\nmatch llm_provider.embed(texts_to_embed).await {\n    Ok(embeddings) =&gt; {\n        for (i, embedding_vector) in embeddings.iter().enumerate() {\n            println!(\"Embedding for text {}: [{} dimensions]\", i + 1, embedding_vector.len());\n            // You would typically store these vectors in a vector database\n            // or use them for similarity calculations.\n            // e.g., println!(\"{:?}\", embedding_vector);\n        }\n        // Example: Check similarity (conceptual, actual similarity requires a function)\n        // let similarity_score = cosine_similarity(&amp;embeddings[0], &amp;embeddings[1]);\n        // println!(\"Similarity between text 1 and 2: {}\", similarity_score);\n    }\n    Err(e) =&gt; {\n        eprintln!(\"Error generating embeddings: {}\", e);\n    }\n}\n</code></pre>"},{"location":"core/embeddings/#use-cases-for-embeddings","title":"Use Cases for Embeddings","text":"<p>Embeddings are a cornerstone of many modern NLP applications, including:</p> <ul> <li>Semantic Search: Finding documents or text snippets that are semantically similar to a query, rather than just matching keywords.</li> <li>Recommendation Systems: Recommending items (articles, products, etc.) similar to what a user has previously interacted with.</li> <li>Clustering: Grouping similar documents or texts together.</li> <li>Classification: Training machine learning models to classify text based on its semantic content.</li> <li>Anomaly Detection: Identifying outliers in textual data.</li> <li>Question Answering: Finding relevant passages in a knowledge base to answer user questions.</li> </ul> <p>QueryMT's <code>EmbeddingProvider</code> trait allows you to leverage different LLMs for embedding generation, enabling you to choose models optimized for this task or experiment with various embedding dimensions and characteristics.</p>"},{"location":"core/providers/","title":"LLM Providers","text":"<p>At the core of QueryMT's design is the concept of an LLM Provider. A provider represents a specific Large Language Model service or backend that QueryMT can interact with. This could be a commercial API like OpenAI's GPT models, Anthropic's Claude, a self-hosted open-source model, or even a custom model accessed via a proprietary API.</p>"},{"location":"core/providers/#the-llmprovider-trait","title":"The <code>LLMProvider</code> Trait","text":"<p>The primary interface for all providers is the <code>LLMProvider</code> trait (defined in <code>crates/querymt/src/lib.rs</code>). This trait unifies the different ways one might interact with an LLM, requiring implementers to support:</p> <ul> <li>Chat Interactions: Via <code>BasicChatProvider</code> and <code>ToolChatProvider</code> supertraits.</li> <li>Text Completion: Via the <code>CompletionProvider</code> supertrait.</li> <li>Embeddings Generation: Via the <code>EmbeddingProvider</code> supertrait.</li> </ul> <p>It also includes optional methods related to tool usage: *   <code>tools()</code>: Returns a list of tools the provider is aware of or configured with. *   <code>call_tool()</code>: Allows the system to invoke a tool call identified by the LLM.</p> <p>By conforming to this trait, different LLM backends can be used interchangeably within QueryMT applications.</p>"},{"location":"core/providers/#http-based-providers","title":"HTTP-Based Providers","text":"<p>Many LLM services are accessed via HTTP APIs. QueryMT provides a specialized trait for these:</p> <ul> <li><code>HTTPLLMProvider</code>: This trait (defined in <code>crates/querymt/src/lib.rs</code>) is implemented by providers that communicate over HTTP. It defines methods for:<ul> <li>Constructing HTTP requests for chat, completion, and embedding operations (e.g., <code>chat_request</code>, <code>complete_request</code>, <code>embed_request</code>).</li> <li>Parsing HTTP responses back into QueryMT's standard data structures (e.g., <code>parse_chat</code>, <code>parse_complete</code>, <code>parse_embed</code>).</li> </ul> </li> </ul> <p>An <code>HTTPLLMProvider</code> is typically wrapped by <code>LLMProviderFromHTTP</code> (from <code>crates/querymt/src/adapters.rs</code>) to make it usable as a full <code>LLMProvider</code>. The adapter handles the actual outbound HTTP call and then uses the <code>HTTPLLMProvider</code>'s parsing methods.</p>"},{"location":"core/providers/#instantiation-and-configuration","title":"Instantiation and Configuration","text":"<p>You don't usually interact with these traits directly to create provider instances. Instead, QueryMT offers:</p> <ol> <li><code>LLMBuilder</code>: A fluent interface to configure and build <code>LLMProvider</code> instances. You specify the provider name (e.g., \"openai\"), model, API keys, and other parameters.</li> <li>Plugin System: For providers not built directly into QueryMT, a plugin system allows new providers to be added dynamically. Plugins implement <code>LLMProviderFactory</code> or <code>HTTPLLMProviderFactory</code> which are then used by the <code>LLMBuilder</code> to create provider instances.</li> </ol> <p>This separation of concerns\u2014the core provider traits, HTTP-specific handling, builder for configuration, and a plugin system for extensibility\u2014makes QueryMT a flexible framework for working with a diverse range of LLMs.</p>"},{"location":"core/tools/","title":"Tools &amp; Function Calling","text":"<p>One of the powerful features of modern Large Language Models is their ability to use \"tools\" or \"call functions.\" This allows LLMs to interact with external systems, APIs, or data sources to gather information or perform actions, making them much more capable and grounded in real-world data. QueryMT provides robust support for defining and using tools.</p>"},{"location":"core/tools/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p><code>querymt::chat::Tool</code>: A struct representing a tool that the LLM can use. It primarily describes a function.</p> <ul> <li><code>tool_type</code>: Currently, this is typically <code>\"function\"</code>.</li> <li><code>function</code>: A <code>querymt::chat::FunctionTool</code> detailing the function.</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> <li> <p><code>querymt::chat::FunctionTool</code>: Describes a specific function the LLM can call.</p> <ul> <li><code>name</code>: The name of the function.</li> <li><code>description</code>: A natural language description of what the function does, its parameters, and when to use it. This is crucial for the LLM to understand the tool's purpose.</li> <li><code>parameters</code>: A <code>serde_json::Value</code> defining the expected input arguments for the function, typically in JSON Schema format.</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> <li> <p><code>querymt::ToolCall</code>: When an LLM decides to use a tool, its response will include one or more <code>ToolCall</code> objects.</p> <ul> <li><code>id</code>: A unique ID for this specific tool call instance.</li> <li><code>call_type</code>: Usually <code>\"function\"</code>.</li> <li><code>function</code>: A <code>querymt::FunctionCall</code>.</li> <li>Source: <code>crates/querymt/src/lib.rs</code></li> </ul> </li> <li> <p><code>querymt::FunctionCall</code>: Details of the function the LLM wants to invoke.</p> <ul> <li><code>name</code>: The name of the function to call.</li> <li><code>arguments</code>: A string containing the arguments for the function, typically as a JSON object.</li> <li>Source: <code>crates/querymt/src/lib.rs</code></li> </ul> </li> <li> <p><code>querymt::chat::ToolChoice</code>: An enum that allows you to specify how the LLM should use the provided tools.</p> <ul> <li><code>Auto</code>: The model can choose to call a tool or not (default).</li> <li><code>Any</code>: The model must call at least one of the available tools.</li> <li><code>Tool(name)</code>: The model must call the specific tool with the given name.</li> <li><code>None</code>: The model is forbidden from calling any tools.</li> <li>Source: <code>crates/querymt/src/chat/mod.rs</code></li> </ul> </li> <li> <p><code>querymt::tool_decorator::CallFunctionTool</code>: A trait that your host-side Rust code must implement for each function you want to make available to the LLM.</p> <ul> <li><code>descriptor()</code>: Returns the <code>Tool</code> definition (schema) for this function.</li> <li><code>call(&amp;self, args: Value)</code>: The actual Rust async function that gets executed when the LLM calls this tool. It receives parsed JSON arguments and should return a string result.</li> <li>Source: <code>crates/querymt/src/tool_decorator.rs</code></li> </ul> </li> <li> <p><code>querymt::tool_decorator::ToolEnabledProvider</code>: A decorator struct that wraps an <code>LLMProvider</code>. When you register tools using <code>LLMBuilder::add_tool()</code>, the builder automatically wraps the base provider with <code>ToolEnabledProvider</code>. This wrapper manages the registered tools and handles the two-way communication:</p> <ol> <li>It passes the tool descriptors to the LLM during a <code>chat_with_tools</code> call.</li> <li>If the LLM responds with a <code>ToolCall</code>, <code>ToolEnabledProvider</code> can dispatch the call to the appropriate <code>CallFunctionTool</code> implementation via its <code>call_tool</code> method.</li> <li>Source: <code>crates/querymt/src/tool_decorator.rs</code></li> </ol> </li> </ul>"},{"location":"core/tools/#workflow","title":"Workflow","text":"<ol> <li> <p>Define Tools:</p> <ul> <li>Implement the <code>CallFunctionTool</code> trait for each Rust function you want to expose.</li> <li>In the <code>descriptor()</code> method, accurately describe the function's purpose and parameters using <code>Tool</code> and <code>FunctionTool</code>.</li> </ul> </li> <li> <p>Register Tools:</p> <ul> <li>When building your <code>LLMProvider</code> using <code>LLMBuilder</code>, use the <code>add_tool()</code> method to register instances of your <code>CallFunctionTool</code> implementations.</li> </ul> </li> <li> <p>Chat with Tools:</p> <ul> <li>Use the <code>chat_with_tools()</code> method on the <code>LLMProvider</code>. The <code>ToolEnabledProvider</code> (if tools were added) will automatically pass the descriptors of registered tools to the LLM.</li> <li>You can use <code>ToolChoice</code> to guide the LLM's tool usage.</li> </ul> </li> <li> <p>LLM Decides to Call a Tool:</p> <ul> <li>The LLM, based on the conversation and tool descriptions, might decide to call one or more tools. Its response (via <code>ChatResponse::tool_calls()</code>) will contain <code>ToolCall</code> objects.</li> </ul> </li> <li> <p>Application Executes Tool:</p> <ul> <li>Your application receives the <code>ToolCall</code>s.</li> <li>The <code>LLMProvider</code> itself (if it's a <code>ToolEnabledProvider</code>) can handle the dispatch via its <code>call_tool(name, args)</code> method. This involves:<ul> <li>Parsing the <code>arguments</code> string (usually JSON) into the expected types for your Rust function.</li> <li>Calling the actual Rust function logic.</li> </ul> </li> </ul> </li> <li> <p>Return Tool Result to LLM:</p> <ul> <li>For each tool call that was executed, create a corresponding <code>ToolCall</code> struct that contains the result. The result string is placed into the <code>function.arguments</code> field.</li> <li>Construct a new <code>ChatMessage</code> using the builder: <code>ChatMessage::user().tool_result(vec_of_result_tool_calls).build()</code>.</li> <li>Send this message (along with the conversation history) back to the LLM using <code>chat_with_tools()</code>.</li> </ul> </li> <li> <p>LLM Continues:</p> <ul> <li>The LLM uses the tool's output to formulate its final response or decide on further actions.</li> </ul> </li> </ol>"},{"location":"core/tools/#example-conceptual-callfunctiontool-implementation","title":"Example (Conceptual <code>CallFunctionTool</code> Implementation)","text":"<pre><code>use querymt::tool_decorator::CallFunctionTool;\nuse querymt::chat::{Tool, FunctionTool};\nuse querymt::builder::FunctionBuilder;\nuse async_trait::async_trait;\nuse serde_json::{Value, json};\n\nstruct GetWeatherTool;\n\n#[async_trait]\nimpl CallFunctionTool for GetWeatherTool {\n    fn descriptor(&amp;self) -&gt; Tool {\n        FunctionBuilder::new(\"get_current_weather\")\n            .description(\"Get the current weather in a given location\")\n            .json_schema(json!({\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }))\n            .build()\n    }\n\n    async fn call(&amp;self, args: Value) -&gt; anyhow::Result&lt;String&gt; {\n        let location = args.get(\"location\").and_then(Value::as_str).unwrap_or_default();\n        // In a real scenario, call a weather API here\n        Ok(json!({ \"weather\": format!(\"Sunny in {}\", location) }).to_string())\n    }\n}\n\n// To use it:\n// let builder = LLMBuilder::new().provider(\"some_provider\").add_tool(GetWeatherTool);\n// let llm = builder.build(&amp;registry)?;\n// ... then use llm.chat_with_tools(...) ...\n</code></pre> <p>Tool usage significantly enhances the capabilities of LLMs, allowing them to perform complex tasks that require external information or actions. QueryMT's system provides a structured way to integrate these tools into your LLM applications.</p>"},{"location":"plugins/","title":"QueryMT Plugins Overview","text":"<p>Welcome to the documentation for QueryMT's plugin system. This system allows you to extend QueryMT's capabilities by integrating various Large Language Model (LLM) providers through two distinct plugin types: Native and Extism (Wasm).</p>"},{"location":"plugins/#two-types-of-plugins","title":"Two Types of Plugins","text":"<p>QueryMT supports a flexible plugin architecture to meet different needs for performance, security, and portability.</p>"},{"location":"plugins/#1-native-plugins","title":"1. Native Plugins","text":"<ul> <li>Mechanism: Native plugins are dynamic shared libraries (<code>.so</code>, <code>.dll</code>, <code>.dylib</code>) that are loaded directly by the QueryMT host.</li> <li>Language: Primarily developed in Rust to directly implement the required provider traits.</li> <li>Pros:<ul> <li>Maximum Performance: Code runs natively with no sandbox overhead, enabling direct memory access and function calls.</li> <li>Simplicity: For Rust developers, this is a very direct integration path.</li> </ul> </li> <li>Cons:<ul> <li>No Sandboxing: The plugin runs with the same permissions as the host application, posing a potential security risk if the plugin is not from a trusted source.</li> <li>Platform-Dependent: A library compiled for <code>linux-x86_64</code> will not run on <code>darwin-aarch64</code>.</li> </ul> </li> </ul>"},{"location":"plugins/#2-extism-wasm-plugins","title":"2. Extism (Wasm) Plugins","text":"<ul> <li>Mechanism: Plugins are WebAssembly (Wasm) modules that run inside a secure sandbox provided by Extism.</li> <li>Language: Can be developed in any language that compiles to Wasm (e.g., Rust, Go, C++, Zig).</li> <li>Pros:<ul> <li>Security: The Wasm sandbox isolates the plugin from the host system, preventing unauthorized file or network access.</li> <li>Portability: A single <code>.wasm</code> file can run on any platform and architecture supported by the host.</li> <li>Dynamic Loading: Add or update LLM providers without recompiling QueryMT.</li> </ul> </li> <li>Cons:<ul> <li>Performance Overhead: Communication between the host and plugin involves serialization (typically to JSON) and crossing the sandbox boundary, which introduces latency compared to a native call.</li> </ul> </li> </ul>"},{"location":"plugins/#core-concepts","title":"Core Concepts","text":"<ul> <li>Plugin: A self-contained unit (either a shared library or a Wasm module) that implements a provider interface.</li> <li>Host: QueryMT itself, which loads, configures, and interacts with plugins via the <code>PluginRegistry</code>.</li> <li>Interface: A defined contract that plugins must adhere to. This differs for Native and Extism plugins but is abstracted away by the host.</li> <li>Configuration: A central file (e.g., <code>plugins.toml</code>) where you define all plugins\u2014both native and Extism\u2014that QueryMT should load.</li> </ul> <p>This documentation will guide you through:</p> <ul> <li>Configuration: How to configure QueryMT to load and use any type of plugin.</li> <li>Plugin Interface: The technical specifications for both Native and Extism plugin interfaces.</li> <li>Plugin Development: How to create your own LLM provider plugins.</li> <li>OCI Plugins: How to distribute and consume plugins using OCI registries.</li> </ul>"},{"location":"plugins/configuration/","title":"Plugin Configuration","text":"<p>QueryMT uses a single configuration file to discover and manage all types of plugins, whether they are Native (shared libraries) or Extism (Wasm). This file tells QueryMT where to find the plugin artifacts and any specific settings they require.</p>"},{"location":"plugins/configuration/#configuration-file-format","title":"Configuration File Format","text":"<p>The configuration can be in TOML, JSON, or YAML format. By default, QueryMT looks for a file named <code>plugins.toml</code>. The host determines the plugin type automatically based on the file extension (<code>.wasm</code>, <code>.so</code>, <code>.dll</code>, <code>.dylib</code>) or from metadata within OCI images.</p>"},{"location":"plugins/configuration/#toml-example-pluginstoml","title":"TOML Example (<code>plugins.toml</code>)","text":"<p>This example shows the configuration for both an Extism/Wasm plugin and a Native plugin.</p> <pre><code># Optional: Configuration for OCI image downloader\n[oci]\ninsecure_skip_signature = false # Set to true to disable signature verification (not recommended for production)\nuse_sigstore_tuf_data = true    # Use official Sigstore TUF data for verification\n# cert_email = \"signer@example.com\"\n# cert_issuer = \"https://github.com/login/oauth\"\n# rekor_pub_keys = \"/path/to/rekor.pub\"\n# fulcio_certs = \"/path/to/fulcio.crt\"\n\n# --- List of provider plugins ---\n\n# Example 1: An Extism (Wasm) plugin loaded from a local file\n[[providers]]\nname = \"my_openai_wasm_plugin\"\npath = \"/path/to/openai_plugin.wasm\"\n# Optional plugin-specific configuration\n[providers.config]\napi_key_env = \"MY_OPENAI_API_KEY\"\nmodel = \"gpt-4\"\ntimeout_ms = 30000\n\n# Example 2: A Native (shared library) plugin\n[[providers]]\nname = \"my_anthropic_native_plugin\"\npath = \"/path/to/anthropic_plugin.so\" # On Linux. Use .dll on Windows, .dylib on macOS\n[providers.config]\napi_key_env = \"MY_ANTHROPIC_API_KEY\"\nmodel = \"claude-3-opus-20240229\"\n\n# Example 3: An Extism (Wasm) plugin from an OCI registry\n[[providers]]\nname = \"secure_oci_plugin\"\npath = \"oci://ghcr.io/my-org/my-plugin:latest\"\n[providers.config]\n# Config specific to this OCI plugin\n</code></pre>"},{"location":"plugins/configuration/#configuration-fields","title":"Configuration Fields","text":""},{"location":"plugins/configuration/#root-level","title":"Root Level","text":"<ul> <li><code>oci</code> (Optional, Object): Configuration for downloading and verifying plugins from OCI registries. See OCI Plugins for details.</li> </ul>"},{"location":"plugins/configuration/#providers-array-of-objects","title":"<code>providers</code> (Array of Objects)","text":"<p>Each object in the <code>providers</code> array defines a single plugin instance:</p> <ul> <li><code>name</code> (String, Required): A unique identifier for this plugin instance. This name is used to select the provider in QueryMT's <code>LLMBuilder</code>.</li> <li><code>path</code> (String, Required): Specifies the location of the plugin module. It can be:<ul> <li>A local file system path to a Wasm module (e.g., <code>/path/to/plugin.wasm</code>).</li> <li>A local file system path to a native shared library (e.g., <code>/path/to/plugin.so</code>, <code>C:\\plugins\\plugin.dll</code>).</li> <li>An HTTP/HTTPS URL (e.g., <code>https://example.com/plugin.wasm</code>).</li> <li>An OCI image reference (e.g., <code>oci://ghcr.io/user/plugin:latest</code>).</li> </ul> </li> <li><code>config</code> (Object, Optional): A TOML table (or JSON object/YAML map) containing plugin-specific configuration. The structure of this object must match what the plugin expects, which can be validated against the plugin's <code>config_schema()</code>. This configuration is passed to the plugin during initialization.</li> </ul>"},{"location":"plugins/data_structures/","title":"Data Structures","text":"<p>These are the primary data structures used in the interface between the QueryMT host and Extism plugins. They are typically serialized to/from JSON.</p> <p>Many of these types are defined in <code>querymt::chat</code>, <code>querymt::completion</code>, and <code>querymt::plugin::extism_impl::interface</code>.</p>"},{"location":"plugins/data_structures/#core-requestresponse-wrappers","title":"Core Request/Response Wrappers","text":"<p>These structures are used as the direct input/output for the main plugin functions (<code>chat</code>, <code>embed</code>, <code>complete</code>). They bundle the plugin-specific configuration (<code>cfg</code>) with the actual request payload.</p> <ul> <li> <p><code>querymt::plugin::extism_impl::ExtismChatRequest&lt;C&gt;</code></p> <ul> <li><code>cfg: C</code>: Plugin-specific configuration (type <code>C</code> is your plugin's config struct).</li> <li><code>messages: Vec&lt;ChatMessage&gt;</code>: The history of chat messages.</li> <li><code>tools: Option&lt;Vec&lt;Tool&gt;&gt;</code>: Optional list of tools the model can use.</li> </ul> </li> <li> <p><code>querymt::plugin::extism_impl::ExtismEmbedRequest&lt;C&gt;</code></p> <ul> <li><code>cfg: C</code>: Plugin-specific configuration.</li> <li><code>inputs: Vec&lt;String&gt;</code>: List of texts to embed.</li> </ul> </li> <li> <p><code>querymt::plugin::extism_impl::ExtismCompleteRequest&lt;C&gt;</code></p> <ul> <li><code>cfg: C</code>: Plugin-specific configuration.</li> <li><code>req: CompletionRequest</code>: The core completion request.</li> </ul> </li> <li> <p><code>querymt::plugin::extism_impl::ExtismChatResponse</code> (implements <code>querymt::chat::ChatResponse</code>)</p> <ul> <li><code>text: Option&lt;String&gt;</code>: The main textual response from the LLM.</li> <li><code>tool_calls: Option&lt;Vec&lt;ToolCall&gt;&gt;</code>: If the LLM decides to call tools.</li> <li><code>thinking: Option&lt;String&gt;</code>: Optional intermediate \"thinking\" messages.</li> <li><code>usage: Option&lt;Usage&gt;</code>: Optional token usage statistics.</li> </ul> </li> </ul>"},{"location":"plugins/data_structures/#common-data-types","title":"Common Data Types","text":"<p>These are used within the request/response wrappers.</p> <ul> <li> <p><code>querymt::chat::ChatMessage</code></p> <ul> <li><code>role: ChatRole</code>: Enum (<code>User</code>, <code>Assistant</code>).</li> <li><code>message_type: MessageType</code>: An enum that determines the message content type.</li> <li><code>content: String</code>: The primary text content of the message.</li> <li>Note on Tool Calls: Tool-related information is carried within the <code>message_type</code> field.<ul> <li><code>MessageType::ToolUse(Vec&lt;ToolCall&gt;)</code>: Used in an <code>Assistant</code> message to indicate the model's decision to call tools.</li> <li><code>MessageType::ToolResult(Vec&lt;ToolCall&gt;)</code>: Used in a <code>User</code> message to provide the results of tool executions back to the model. In this case, the <code>arguments</code> field of each <code>ToolCall</code> contains the JSON string result of the function.</li> </ul> </li> </ul> </li> <li> <p><code>querymt::chat::Tool</code></p> <ul> <li><code>tool_type: String</code>: Typically <code>\"function\"</code>.</li> <li><code>function: FunctionTool</code>: Describes the function tool.</li> </ul> </li> <li> <p><code>querymt::chat::FunctionTool</code></p> <ul> <li><code>name: String</code>: Name of the function.</li> <li><code>description: String</code>: Description of the function.</li> <li><code>parameters: serde_json::Value</code>: JSON schema defining the function's parameters.</li> </ul> </li> <li> <p><code>querymt::ToolCall</code></p> <ul> <li><code>id: String</code>: Unique ID for the tool call.</li> <li><code>call_type: String</code>: Typically <code>\"function\"</code>.</li> <li><code>function: FunctionCall</code>: Details of the function to be called.</li> </ul> </li> <li> <p><code>querymt::FunctionCall</code></p> <ul> <li><code>name: String</code>: Name of the function called.</li> <li><code>arguments: String</code>: JSON string of arguments for the function (or the result of the function, in a <code>ToolResult</code> message).</li> </ul> </li> <li> <p><code>querymt::completion::CompletionRequest</code></p> <ul> <li><code>prompt: String</code>: The prompt for completion.</li> <li><code>suffix: Option&lt;String&gt;</code></li> <li><code>max_tokens: Option&lt;u32&gt;</code></li> <li><code>temperature: Option&lt;f32&gt;</code></li> </ul> </li> <li> <p><code>querymt::completion::CompletionResponse</code></p> <ul> <li><code>text: String</code>: The completed text.</li> </ul> </li> </ul> <p>For plugin developers using Rust, these types are readily available. When interacting via raw JSON, ensure your data conforms to these structures. The <code>config_schema()</code> export of a plugin should define the structure for the generic <code>C</code> (plugin configuration) part of the <code>Extism*Request</code> types.</p>"},{"location":"plugins/development/","title":"Plugin Development","text":"<p>This guide provides instructions for developing both Native and Extism (Wasm) plugins for QueryMT.</p>"},{"location":"plugins/development/#developing-native-plugins","title":"Developing Native Plugins","text":"<p>Native plugins offer the best performance by running as shared libraries directly within the host process. They are recommended for trusted, performance-critical integrations.</p>"},{"location":"plugins/development/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Rust Toolchain: Install Rust from rust-lang.org.</li> </ul>"},{"location":"plugins/development/#2-project-setup","title":"2. Project Setup","text":"<ol> <li> <p>Create a new Rust library project:     <pre><code>cargo new my_native_plugin --lib\ncd my_native_plugin\n</code></pre></p> </li> <li> <p>Update <code>Cargo.toml</code>:     Configure the crate to build a dynamic system library (<code>cdylib</code>).</p> <p><pre><code>[package]\nname = \"my_native_plugin\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[lib]\ncrate-type = [\"cdylib\"] # Important for shared libraries\n\n[dependencies]\nquerymt = { path = \"../../..\", features = [\"http-client\"] } # Adjust path\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\nschemars = \"0.8\"\nhttp = \"0.2\"\n# Add any other dependencies your provider needs\n</code></pre> Note: The <code>querymt</code> dependency does not use the <code>extism_plugin</code> feature.</p> </li> </ol>"},{"location":"plugins/development/#3-implementing-the-plugin","title":"3. Implementing the Plugin","text":"<p>You will implement the <code>HTTPLLMProviderFactory</code> trait and export it via the <code>plugin_http_factory</code> function.</p> <pre><code>// src/lib.rs\nuse querymt::plugin::http::{HTTPLLMProviderFactory, HTTPFactoryCtor};\nuse querymt::chat::http::HTTPChatProvider;\nuse querymt::completion::http::HTTPCompletionProvider;\nuse querymt::embedding::http::HTTPEmbeddingProvider;\nuse querymt::{\n    HTTPLLMProvider, CompletionRequest, CompletionResponse,\n    ChatMessage, ChatResponse, Tool, ToolCall, LLMError\n};\nuse serde::{Serialize, Deserialize};\nuse schemars::{JsonSchema, schema_for};\nuse std::collections::HashMap;\nuse std::error::Error;\n\n// 1. Define your plugin's configuration structure\n#[derive(Serialize, Deserialize, JsonSchema, Clone, Debug)]\npub struct MyPluginConfig {\n    pub api_key: String,\n    pub model_name: Option&lt;String&gt;,\n    #[serde(default = \"default_base_url\")]\n    pub base_url: String,\n}\nfn default_base_url() -&gt; String { \"https://api.examplellm.com/v1\".to_string() }\n\n// 2. Define your provider struct. It holds the config.\n#[derive(Clone)]\npub struct MyProvider {\n    config: MyPluginConfig,\n}\n\n// 3. Implement the core HTTP provider traits for your provider struct.\n// This defines how to build requests and parse responses.\nimpl HTTPChatProvider for MyProvider {\n    // ... implement chat_request() and parse_chat() ...\n    fn chat_request(&amp;self, messages: &amp;[ChatMessage], _tools: Option&lt;&amp;[Tool]&gt;) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, LLMError&gt; { /* ... */ Ok(http::Request::default()) }\n    fn parse_chat(&amp;self, resp: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Box&lt;dyn ChatResponse&gt;, Box&lt;dyn Error&gt;&gt; { /* ... */ Ok(Box::new(querymt::completion::CompletionResponse{text:\"...\".into()})) }\n}\nimpl HTTPEmbeddingProvider for MyProvider { /* ... */\n    fn embed_request(&amp;self, inputs: &amp;[String]) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, LLMError&gt; { Ok(http::Request::default()) }\n    fn parse_embed(&amp;self, resp: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Vec&lt;Vec&lt;f32&gt;&gt;, Box&lt;dyn Error&gt;&gt; { Ok(vec![]) }\n}\nimpl HTTPCompletionProvider for MyProvider { /* ... */\n    fn complete_request(&amp;self, req: &amp;CompletionRequest) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, LLMError&gt; { Ok(http::Request::default()) }\n    fn parse_complete(&amp;self, resp: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;CompletionResponse, Box&lt;dyn Error&gt;&gt; { Ok(CompletionResponse{text:\"...\".into()}) }\n}\n\n// This blanket impl turns your provider struct into an HTTPLLMProvider\nimpl HTTPLLMProvider for MyProvider {}\n\n// 4. Implement the factory, which knows how to create your provider.\npub struct MyFactory;\n\nimpl HTTPLLMProviderFactory for MyFactory {\n    fn name(&amp;self) -&gt; &amp;str {\n        \"My Native HTTP Plugin\"\n    }\n\n    fn config_schema(&amp;self) -&gt; serde_json::Value {\n        serde_json::to_value(schema_for!(MyPluginConfig)).unwrap()\n    }\n\n    fn from_config(&amp;self, cfg: &amp;serde_json::Value) -&gt; Result&lt;Box&lt;dyn HTTPLLMProvider&gt;, Box&lt;dyn Error&gt;&gt; {\n        let config: MyPluginConfig = serde_json::from_value(cfg.clone())?;\n        let provider = MyProvider { config };\n        Ok(Box::new(provider))\n    }\n\n    // Implement list_models_request, parse_list_models, api_key_name...\n    fn list_models_request(&amp;self, cfg: &amp;serde_json::Value) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, LLMError&gt; { Ok(http::Request::default()) }\n    fn parse_list_models(&amp;self, resp: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Vec&lt;String&gt;, Box&lt;dyn Error&gt;&gt; { Ok(vec![]) }\n}\n\n// 5. Export the factory constructor function. This is the entry point for the host.\n#[no_mangle]\npub unsafe extern \"C\" fn plugin_http_factory() -&gt; *mut dyn HTTPLLMProviderFactory {\n    Box::into_raw(Box::new(MyFactory))\n}\n</code></pre>"},{"location":"plugins/development/#4-building-the-plugin","title":"4. Building the Plugin","text":"<p>Compile your Rust library into a shared object: <pre><code>cargo build --release\n</code></pre> The compiled library will be at <code>target/release/libmy_native_plugin.so</code> (or <code>.dll</code>/<code>.dylib</code>). This is the file you configure in <code>plugins.toml</code>.</p>"},{"location":"plugins/development/#developing-extism-wasm-plugins","title":"Developing Extism (Wasm) Plugins","text":"<p>Extism plugins provide security and portability by running in a Wasm sandbox.</p>"},{"location":"plugins/development/#1-prerequisites_1","title":"1. Prerequisites","text":"<ul> <li>Rust Toolchain: Install Rust from rust-lang.org.</li> <li>Wasm Target: Add the Wasm target: <code>rustup target add wasm32-wasip1</code>.</li> </ul>"},{"location":"plugins/development/#2-project-setup_1","title":"2. Project Setup","text":"<ol> <li> <p>Create a new Rust library project:     <pre><code>cargo new my_extism_plugin --lib\ncd my_extism_plugin\n</code></pre></p> </li> <li> <p>Update <code>Cargo.toml</code>:     <pre><code>[package]\nname = \"my_extism_plugin\"\n# ...\n\n[lib]\ncrate-type = [\"cdylib\"]\n\n[dependencies]\nextism-pdk = \"1.0.0\"\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\nschemars = \"0.8\"\nquerymt = { path = \"../../..\", features = [\"extism_plugin\"] } # Note the feature\n\n[profile.release]\nlto = true\nopt-level = 'z'\nstrip = true\n</code></pre></p> </li> </ol>"},{"location":"plugins/development/#3-implementing-the-plugin_1","title":"3. Implementing the Plugin","text":"<p>The easiest way to create an HTTP-based Wasm plugin is using the <code>impl_extism_http_plugin!</code> macro. The implementation logic for the HTTP traits is identical to the native plugin example, but it's all wrapped in the macro.</p> <pre><code>// src/lib.rs\nuse querymt::plugin::extism_impl::impl_extism_http_plugin;\nuse querymt::chat::http::HTTPChatProvider;\nuse querymt::plugin::http::HTTPLLMProviderFactory;\n// ... other trait imports\n\n// 1. Define your config struct (same as native example)\n#[derive(serde::Serialize, serde::Deserialize, schemars::JsonSchema, Clone, Debug)]\npub struct MyPluginConfig { /* ... */ }\n// ... with default_base_url() function\n\n// 2. Implement the HTTP provider traits for your config struct\nimpl HTTPChatProvider for MyPluginConfig { /* ... */ }\n// ... HTTPEmbeddingProvider, HTTPCompletionProvider ...\n\n// 3. Create a marker struct for your factory logic\nstruct MyPluginFactory;\n\n// 4. Implement the factory trait for the marker struct\nimpl HTTPLLMProviderFactory for MyPluginFactory { /* ... */ }\n\n// 5. Use the macro to export all necessary Extism functions\nimpl_extism_http_plugin!(\n    config = MyPluginConfig,\n    factory = MyPluginFactory,\n    name = \"My Example Extism HTTP Plugin\"\n);\n</code></pre>"},{"location":"plugins/development/#4-building-the-plugin_1","title":"4. Building the Plugin","text":"<p>Compile your Rust library to Wasm: <pre><code>cargo build --target wasm32-wasip1 --release\n</code></pre> The Wasm file will be at <code>target/wasm32-wasip1/release/my_extism_plugin.wasm</code>. This is the file you configure in <code>plugins.toml</code>.</p>"},{"location":"plugins/helper_macros/","title":"Plugin Development Helper Macros","text":"<p>QueryMT provides macros to simplify the development of Extism plugins in Rust, especially for common patterns like HTTP-based LLM providers.</p> <p>Note: The macros described on this page are designed exclusively for the development of Extism (Wasm) plugins. They handle the boilerplate of creating Wasm exports and managing data serialization across the sandbox boundary.</p> <p>For Native Plugin development, you should implement the <code>LLMProviderFactory</code> or <code>HTTPLLMProviderFactory</code> traits directly, as described in the Native Plugin Development Guide.</p>"},{"location":"plugins/helper_macros/#impl_binary_codectype","title":"<code>impl_binary_codec!($Type)</code>","text":"<ul> <li>Source: <code>crates/querymt/src/plugin/extism_impl/wrapper.rs</code></li> <li>Purpose: Implements the <code>BinaryCodec</code> and <code>FromBytesOwned</code> traits for a given struct. This enables easy serialization to and deserialization from JSON byte arrays, which is the primary way data is exchanged between the Extism host and plugin.</li> <li>Usage:     <pre><code>use querymt::plugin::extism_impl::wrapper::BinaryCodec;\nuse querymt::impl_binary_codec;\nuse serde::{Serialize, Deserialize};\n\n#[derive(Serialize, Deserialize)]\nstruct MyData {\n    field: String,\n}\nimpl_binary_codec!(MyData);\n\n#[derive(Serialize, Deserialize)]\nstruct MyGenericData&lt;T&gt; {\n    data: T,\n}\nimpl_binary_codec!(MyGenericData&lt;T&gt;); // For generic types\n</code></pre></li> <li>Details:<ul> <li>The macro has two forms: one for non-generic types (e.g., <code>MyData</code>) and one for types with a single generic parameter (e.g., <code>MyGenericData&lt;C&gt;</code>).</li> <li>It relies on <code>serde_json</code> for serialization/deserialization.</li> <li>The types <code>ExtismChatRequest&lt;C&gt;</code>, <code>ExtismEmbedRequest&lt;C&gt;</code>, and <code>ExtismCompleteRequest&lt;C&gt;</code> already have this implemented in <code>querymt</code>.</li> </ul> </li> </ul>"},{"location":"plugins/helper_macros/#impl_extism_http_plugin","title":"<code>impl_extism_http_plugin!</code>","text":"<ul> <li>Source: <code>crates/querymt/src/plugin/extism_impl/wrapper.rs</code></li> <li>Purpose: Generates all the necessary Extism plugin exports (<code>#[plugin_fn]</code>) for an LLM provider that interacts with an external service via HTTP. This significantly reduces boilerplate code.</li> <li>Usage:     <pre><code>use querymt::impl_extism_http_plugin;\nuse serde::{Serialize, Deserialize};\nuse schemars::JsonSchema;\n// ... other necessary imports for your provider traits ...\n# use querymt::chat::http::HTTPChatProvider;\n# use querymt::completion::http::HTTPCompletionProvider;\n# use querymt::embedding::http::HTTPEmbeddingProvider;\n# use querymt::plugin::http::HTTPLLMProviderFactory;\n# use querymt::{CompletionRequest, CompletionResponse, ChatMessage, ChatResponse, Tool, ToolCall, Usage};\n\n#[derive(Serialize, Deserialize, JsonSchema, Clone, Debug)]\npub struct MyPluginConfig { /* ... fields ... */\n# pub api_key: String, pub base_url: String, pub model_name: Option&lt;String&gt;,\n}\n# impl MyPluginConfig { fn default_base_url() -&gt; String { \"\".to_string() } }\n# impl HTTPChatProvider for MyPluginConfig { /* ... */ fn chat_request(&amp;self, _: &amp;[ChatMessage], _: Option&lt;&amp;[Tool]&gt;) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, querymt::error::LLMError&gt; { todo!() } fn parse_chat(&amp;self, _: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Box&lt;dyn ChatResponse&gt;, Box&lt;dyn std::error::Error&gt;&gt; { todo!() } }\n# impl HTTPEmbeddingProvider for MyPluginConfig { /* ... */ fn embed_request(&amp;self, _: &amp;[String]) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, querymt::error::LLMError&gt; { todo!() } fn parse_embed(&amp;self, _: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Vec&lt;Vec&lt;f32&gt;&gt;, Box&lt;dyn std::error::Error&gt;&gt; { todo!() } }\n# impl HTTPCompletionProvider for MyPluginConfig { /* ... */ fn complete_request(&amp;self, _: &amp;CompletionRequest) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, querymt::error::LLMError&gt; { todo!() } fn parse_complete(&amp;self, _: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;CompletionResponse, Box&lt;dyn std::error::Error&gt;&gt; { todo!() } }\n\n\npub struct MyPluginFactory;\nimpl HTTPLLMProviderFactory for MyPluginFactory { /* ... */\n# fn name(&amp;self) -&gt; &amp;str { \"test\" }\n# fn api_key_name(&amp;self) -&gt; Option&lt;String&gt; { None }\n# fn config_schema(&amp;self) -&gt; serde_json::Value { serde_json::Value::Null }\n# fn from_config(&amp;self, _cfg: &amp;serde_json::Value) -&gt; Result&lt;Box&lt;dyn querymt::HTTPLLMProvider&gt;, Box&lt;dyn std::error::Error&gt;&gt; { todo!() }\n# fn list_models_request(&amp;self, _cfg: &amp;serde_json::Value) -&gt; Result&lt;http::Request&lt;Vec&lt;u8&gt;&gt;, querymt::error::LLMError&gt; { todo!() }\n# fn parse_list_models(&amp;self, _resp: http::Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Vec&lt;String&gt;, Box&lt;dyn std::error::Error&gt;&gt; { todo!() }\n}\n\nimpl_extism_http_plugin!(\n    config = MyPluginConfig,         // Your config struct\n    factory = MyPluginFactory,       // Your factory struct implementing HTTPLLMProviderFactory\n    name = \"My Awesome HTTP LLM\"     // Display name for the plugin\n);\n</code></pre></li> <li>Parameters:<ul> <li><code>config = $ConfigType</code>: The Rust struct you define for this plugin's specific configuration (e.g., <code>MyPluginConfig</code>). This struct must derive <code>Serialize</code>, <code>Deserialize</code>, <code>JsonSchema</code>, and <code>Clone</code>. It also needs to implement <code>HTTPChatProvider</code>, <code>HTTPEmbeddingProvider</code>, and <code>HTTPCompletionProvider</code> traits from <code>querymt</code>.</li> <li><code>factory = $FactoryType</code>: A unit struct (or any struct) that implements the <code>HTTPLLMProviderFactory</code> trait from <code>querymt</code>. This factory is responsible for tasks like providing the API key name, constructing HTTP requests for listing models, and parsing their responses.</li> <li><code>name = $PluginNameExpr</code>: A string expression for the human-readable name of the plugin.</li> </ul> </li> <li>Generated Exports:<ul> <li><code>name()</code></li> <li><code>api_key_name()</code> (delegates to <code>$FactoryType::api_key_name()</code>)</li> <li><code>config_schema()</code> (generates JSON schema from <code>$ConfigType</code>)</li> <li><code>from_config()</code> (validates <code>$ConfigType</code> via deserialization)</li> <li><code>list_models()</code> (uses <code>$FactoryType</code> to make HTTP request and parse response)</li> <li><code>base_url()</code> (uses a <code>default_base_url()</code> function required on <code>$ConfigType</code>)</li> <li><code>chat()</code> (uses <code>$ConfigType::chat_request()</code> and <code>$ConfigType::parse_chat()</code>)</li> <li><code>embed()</code> (uses <code>$ConfigType::embed_request()</code> and <code>$ConfigType::parse_embed()</code>)</li> <li><code>complete()</code> (uses <code>$ConfigType::complete_request()</code> and <code>$ConfigType::parse_complete()</code>)</li> </ul> </li> <li>Requirements for <code>$ConfigType</code>:<ul> <li>Must derive <code>serde::Serialize</code>, <code>serde::Deserialize</code>, <code>schemars::JsonSchema</code>, <code>Clone</code>.</li> <li>Must implement a <code>default_base_url() -&gt; String</code> static method.</li> <li>Must implement <code>querymt::chat::http::HTTPChatProvider</code>.</li> <li>Must implement <code>querymt::embedding::http::HTTPEmbeddingProvider</code>.</li> <li>Must implement <code>querymt::completion::http::HTTPCompletionProvider</code>.</li> </ul> </li> <li>Requirements for <code>$FactoryType</code>:<ul> <li>Must implement <code>querymt::plugin::http::HTTPLLMProviderFactory</code>.</li> </ul> </li> </ul>"},{"location":"plugins/interface_spec/","title":"Plugin Interface Specification","text":"<p>To be compatible with QueryMT, a plugin must conform to a specific interface. The interface differs depending on whether you are building a Native Plugin (a shared library) or an Extism Plugin (a Wasm module).</p>"},{"location":"plugins/interface_spec/#1-native-plugin-interface","title":"1. Native Plugin Interface","text":"<p>Native plugins are Rust <code>cdylib</code> crates that implement traits from the <code>querymt</code> library and export a factory function with a C ABI. This allows the host to load the library and construct a provider instance with type safety and high performance.</p>"},{"location":"plugins/interface_spec/#core-traits","title":"Core Traits","text":"<p>Your plugin must implement one of the following factory traits:</p> <ul> <li><code>querymt::plugin::http::HTTPLLMProviderFactory</code>: The recommended and simplest approach for providers that communicate over HTTP. The host will wrap this in an adapter to handle the async HTTP calls.</li> <li><code>querymt::plugin::LLMProviderFactory</code>: A more advanced, fully async trait for providers that have non-standard communication needs or do not use HTTP.</li> </ul>"},{"location":"plugins/interface_spec/#exported-factory-function","title":"Exported Factory Function","text":"<p>Your library must export one of the following C-ABI functions. The host looks for these in order.</p> <ol> <li> <p><code>plugin_http_factory</code> (if you implement <code>HTTPLLMProviderFactory</code>)     <pre><code>use querymt::plugin::http::{HTTPLLMProviderFactory, HTTPFactoryCtor};\n\nstruct MyFactory;\n// ... impl HTTPLLMProviderFactory for MyFactory ...\n\n#[no_mangle]\npub unsafe extern \"C\" fn plugin_http_factory() -&gt; *mut dyn HTTPLLMProviderFactory {\n    Box::into_raw(Box::new(MyFactory))\n}\n</code></pre></p> </li> <li> <p><code>plugin_factory</code> (if you implement <code>LLMProviderFactory</code>)     <pre><code>use querymt::plugin::{LLMProviderFactory, FactoryCtor};\n\nstruct MyAsyncFactory;\n// ... impl LLMProviderFactory for MyAsyncFactory ...\n\n#[no_mangle]\npub unsafe extern \"C\" fn plugin_factory() -&gt; *mut dyn LLMProviderFactory {\n    Box::into_raw(Box::new(MyAsyncFactory))\n}\n</code></pre></p> </li> </ol>"},{"location":"plugins/interface_spec/#httpllmproviderfactory-trait-methods","title":"<code>HTTPLLMProviderFactory</code> Trait Methods","text":"<p>When using the recommended HTTP-based approach, you need to implement these methods:</p> <ul> <li><code>name() -&gt; &amp;str</code>: Returns the display name of the provider.</li> <li><code>config_schema() -&gt; Value</code>: Returns a <code>serde_json::Value</code> representing the JSON schema for the plugin's configuration.</li> <li><code>from_config(&amp;Value) -&gt; Result&lt;Box&lt;dyn HTTPLLMProvider&gt;, Box&lt;dyn Error&gt;&gt;</code>: Validates the user's configuration and creates an instance of your provider struct that implements the <code>HTTPLLMProvider</code> trait.</li> <li><code>list_models_request(&amp;Value) -&gt; Result&lt;Request&lt;Vec&lt;u8&gt;&gt;, LLMError&gt;</code>: Constructs an <code>http::Request</code> to fetch the list of available models.</li> <li><code>parse_list_models(Response&lt;Vec&lt;u8&gt;&gt;) -&gt; Result&lt;Vec&lt;String&gt;, Box&lt;dyn Error&gt;&gt;</code>: Parses the <code>http::Response</code> from the models list request into a vector of model names.</li> <li><code>api_key_name() -&gt; Option&lt;String&gt;</code>: (Optional) Returns the name of an environment variable for an API key.</li> </ul> <p>Your provider struct created in <code>from_config</code> will then need to implement <code>HTTPChatProvider</code>, <code>HTTPEmbeddingProvider</code>, and <code>HTTPCompletionProvider</code>. See the Native Plugin Development Guide for a full example.</p>"},{"location":"plugins/interface_spec/#2-extism-wasm-plugin-interface","title":"2. Extism (Wasm) Plugin Interface","text":"<p>Extism plugins are Wasm modules that export a set of functions that the host calls. Data is passed between the host and plugin as JSON-encoded byte arrays. The <code>impl_extism_http_plugin!</code> macro can generate all of these exports for you.</p>"},{"location":"plugins/interface_spec/#core-exported-functions","title":"Core Exported Functions","text":"<ol> <li><code>name() -&gt; String</code>: Returns the human-readable name of the plugin.</li> <li><code>api_key_name() -&gt; Option&lt;String&gt;</code>: Returns the name of an environment variable for an API key.</li> <li><code>config_schema() -&gt; String</code>: Returns a JSON string representing the JSON Schema for the plugin's configuration.</li> <li><code>from_config(config: Json&lt;YourConfigType&gt;) -&gt; Result&lt;Json&lt;YourConfigType&gt;, Error&gt;</code>: Validates the plugin-specific configuration.</li> <li><code>list_models(config: Json&lt;serde_json::Value&gt;) -&gt; Result&lt;Json&lt;Vec&lt;String&gt;&gt;, Error&gt;</code>: Dynamically lists available models, usually by making an HTTP request from within the Wasm module.</li> <li><code>base_url() -&gt; String</code>: Returns the default base URL for the provider, used by the host to configure network access for the sandbox.</li> </ol>"},{"location":"plugins/interface_spec/#llm-operation-functions","title":"LLM Operation Functions","text":"<p>These functions handle the core LLM tasks. Their inputs are wrapper structs that bundle the configuration with the request data.</p> <ol> <li><code>chat(input: ExtismChatRequest&lt;YourConfigType&gt;) -&gt; Result&lt;Json&lt;ExtismChatResponse&gt;, Error&gt;</code>: Handles chat completion requests.</li> <li><code>embed(input: ExtismEmbedRequest&lt;YourConfigType&gt;) -&gt; Result&lt;Json&lt;Vec&lt;Vec&lt;f32&gt;&gt;&gt;, Error&gt;</code>: Generates embeddings.</li> <li><code>complete(input: ExtismCompleteRequest&lt;YourConfigType&gt;) -&gt; Result&lt;Json&lt;CompletionResponse&gt;, Error&gt;</code>: Handles text completion.</li> </ol> <p>See Data Structures for details on the <code>Extism*</code> request/response types.</p>"},{"location":"plugins/oci_plugins/","title":"OCI Plugins","text":"<p>QueryMT supports loading plugins distributed as OCI (Open Container Initiative) images from container registries like Docker Hub, GitHub Container Registry (GHCR), etc. This provides a standardized way to package and distribute plugins.</p>"},{"location":"plugins/oci_plugins/#distributing-native-and-wasm-plugins","title":"Distributing Native and Wasm Plugins","text":"<p>OCI registries can be used to distribute both Native and Wasm plugins. The QueryMT host determines which type of plugin an image contains using the following logic:</p> <ol> <li>Platform Matching: It first checks the image index for a manifest matching the host's operating system and architecture (e.g., <code>linux/amd64</code>). If a match is found, it's treated as a Native Plugin.</li> <li>Wasm Fallback: If no native platform matches, it looks for a <code>wasi/wasm</code> platform. If found, it's treated as an Extism (Wasm) Plugin.</li> <li>Layer Type/Annotations: As a fallback, it can inspect layer media types (<code>application/vnd.oci.image.layer.v1.tar+gzip</code> for native, <code>application/vnd.wasm.v1.layer+wasm</code> for Wasm) or an annotation (<code>mt.query.plugin.type</code>) to determine the type.</li> </ol> <p>When publishing, ensure your image is built for the correct target platform(s).</p>"},{"location":"plugins/oci_plugins/#configuration","title":"Configuration","text":"<p>To use an OCI plugin, specify its path in the <code>extism_plugins.toml</code> (or equivalent JSON/YAML) configuration file using the <code>oci://</code> prefix:</p> <pre><code>[[providers]]\nname = \"my_oci_plugin\"\npath = \"oci://ghcr.io/my-org/my-plugin:latest\" # OCI image reference\n[providers.config]\n# Plugin-specific configuration\n# ...\n</code></pre>"},{"location":"plugins/oci_plugins/#plugin-caching","title":"Plugin Caching","text":"<p>When QueryMT encounters an <code>oci://</code> path, it will: 1.  Parse the image reference (e.g., <code>ghcr.io/my-org/my-plugin:latest</code>). 2.  Generate a cache key based on the image reference and plugin name. 3.  Check if the plugin Wasm file already exists in the local cache directory (typically <code>~/.cache/querymt/</code> or platform equivalent). 4.  If not cached:     a.  Pull the OCI image layers.     b.  Attempt to extract a file named <code>plugin.wasm</code> (or a configured target file path, though <code>plugin.wasm</code> is the default assumption for generic OCI plugins) from the image layers.     c.  Store the extracted Wasm file in the cache directory. 5.  Load the Wasm plugin from the cached file.</p> <p>This means subsequent loads of the same plugin version will be much faster as they use the local cache.</p>"},{"location":"plugins/oci_plugins/#signature-verification-sigstore-cosign","title":"Signature Verification (Sigstore Cosign)","text":"<p>For enhanced security, QueryMT can verify OCI image signatures using Sigstore Cosign before pulling and extracting the plugin. This helps ensure the plugin's authenticity and integrity.</p> <p>Signature verification is configured in the optional <code>[oci]</code> section of the <code>extism_plugins.toml</code> file:</p> <pre><code>[oci]\n# If 'true', skips signature verification. Defaults to 'false'.\n# Not recommended for production environments.\ninsecure_skip_signature = false\n\n# If 'true' (default), uses the official Sigstore TUF root to fetch trusted\n# Fulcio root certificates and Rekor transparency log public keys.\nuse_sigstore_tuf_data = true\n\n# --- Manual Trust Configuration (if 'use_sigstore_tuf_data' is false or for specific overrides) ---\n\n# Path to a file containing Rekor public keys (PEM encoded).\n# rekor_pub_keys = \"/etc/querymt/security/rekor.pub\"\n\n# Path to a file containing Fulcio root certificates (PEM encoded).\n# fulcio_certs = \"/etc/querymt/security/fulcio.crt\"\n\n# --- Verification Constraints ---\n# These act as policies for signature validation.\n\n# Verify the signer's certificate was issued to this specific email address.\n# cert_email = \"signer@example.com\"\n\n# Verify the signer's certificate was issued by this specific OIDC issuer.\n# Often used with cert_email or cert_url.\n# Example: \"https://github.com/login/oauth\" for GitHub OIDC, or \"https://accounts.google.com\" for Google.\n# cert_issuer = \"https://oidc.issuer.example.com\"\n\n# Verify the signer's certificate SAN (Subject Alternative Name) matches this URL.\n# cert_url = \"https://github.com/my-org/my-repo/.github/workflows/release.yml@refs/tags/v1.0.0\"\n</code></pre>"},{"location":"plugins/oci_plugins/#how-verification-works","title":"How Verification Works:","text":"<ol> <li>Enabled by Default: Signature verification is generally enabled (<code>insecure_skip_signature = false</code>).</li> <li>Trust Root:<ul> <li>By default (<code>use_sigstore_tuf_data = true</code>), QueryMT attempts to fetch the latest trust materials (Fulcio CAs, Rekor keys) from the public Sigstore TUF repository.</li> <li>Alternatively, you can provide paths to local files for Rekor public keys and Fulcio certificates if you operate in an air-gapped environment or use a private Sigstore instance.</li> </ul> </li> <li>Constraints: You can specify constraints like <code>cert_email</code>, <code>cert_issuer</code>, or <code>cert_url</code> to ensure the signature was created by an expected identity or build process.</li> <li>Verification Process:<ul> <li>QueryMT (via its OCI client and Sigstore libraries) looks for a signature manifest associated with the plugin image (e.g., <code>ghcr.io/my-org/my-plugin:sha256-digest.sig</code>).</li> <li>It fetches the signature and the associated certificate/identity information.</li> <li>It verifies the signature against the image digest.</li> <li>It verifies the certificate chain against the trusted Fulcio roots.</li> <li>It checks if the signing identity satisfies the configured constraints.</li> <li>It may optionally verify transparency log entries in Rekor.</li> </ul> </li> </ol> <p>If verification fails (e.g., no valid signature found, constraints not met), QueryMT will refuse to load the plugin and log an error.</p>"},{"location":"plugins/oci_plugins/#publishing-oci-plugins","title":"Publishing OCI Plugins","text":"<p>To publish your plugin as an OCI image suitable for QueryMT: 1.  Ensure your Wasm module is named <code>plugin.wasm</code> (or be prepared for users to configure a custom extraction path if QueryMT supports it in the future). 2.  Create a simple <code>Dockerfile</code> or use a tool like <code>crane</code> or <code>oras</code> to package <code>plugin.wasm</code> into an OCI image layer.     Example <code>Dockerfile</code> (very basic, might need adjustment for media types if your registry is strict):     <pre><code>FROM scratch\nCOPY plugin.wasm /plugin.wasm\n</code></pre> 3.  Build and push the image to your chosen OCI registry. 4.  Sign your image using <code>cosign sign &lt;your-image-ref&gt;</code>. This is crucial for users who want to verify plugin authenticity.</p> <p>Users can then refer to your plugin using its OCI reference (e.g., <code>oci://your-registry/your-plugin:tag</code>).</p>"},{"location":"plugins/usage/","title":"Using Plugins with the Host","text":"<p>Once plugins are developed, QueryMT's <code>querymt::plugin::host::PluginRegistry</code> is responsible for loading them from a configuration file and making them available to the application.</p>"},{"location":"plugins/usage/#initialization","title":"Initialization","text":"<p>The registry is typically initialized at application startup by pointing it to a configuration file. You must also register <code>PluginLoader</code> implementations for the types of plugins you want to support (e.g., native shared libraries, Extism/Wasm).</p> <pre><code>use querymt::plugin::host::{PluginRegistry, native::NativeLoader};\nuse querymt::plugin::extism_impl::host::ExtismLoader;\nuse querymt::error::LLMError;\n\n# async fn example() -&gt; Result&lt;(), LLMError&gt; {\n// 1. Create a registry from a config file path.\nlet mut registry = PluginRegistry::from_path(\"plugins.toml\")?;\n\n// 2. Register loaders for the plugin types you want to support.\n#[cfg(feature = \"native\")]\nregistry.register_loader(Box::new(NativeLoader));\n#[cfg(feature = \"extism_host\")]\nregistry.register_loader(Box::new(ExtismLoader));\n\n// 3. Load all plugins defined in the config file.\nregistry.load_all_plugins().await;\n# Ok(())\n# }\n</code></pre> <p>This process will: 1. Parse the configuration file (e.g., <code>plugins.toml</code>). 2. For each configured provider:     - Determine its location (local path, OCI image).     - If it's an OCI image, download and cache it.     - Determine its type (<code>Native</code> or <code>Wasm</code>).     - Use the corresponding registered <code>PluginLoader</code> to load the plugin into memory.     - Create a factory (<code>Arc&lt;dyn LLMProviderFactory&gt;</code>) for the plugin and store it in the registry, keyed by its configured name.</p>"},{"location":"plugins/usage/#accessing-provider-factories","title":"Accessing Provider Factories","text":"<p>Once loaded, you can use the <code>PluginRegistry</code> with <code>LLMBuilder</code> to create provider instances. The builder will use the registry to find the correct factory.</p> <pre><code>use querymt::builder::LLMBuilder;\nuse querymt::plugin::host::PluginRegistry;\nuse querymt::LLMProvider;\nuse serde_json::json;\n# async fn example(registry: PluginRegistry) -&gt; anyhow::Result&lt;()&gt; {\n\n// Use the builder to configure a provider by its name from the config file.\nlet provider: Box&lt;dyn LLMProvider&gt; = LLMBuilder::new()\n    .provider(\"my_openai_plugin\") // This name must match a name in plugins.toml\n    .model(\"gpt-4-turbo\")\n    // The builder automatically sets provider-specific config like api_key\n    // from its own fields. Here we set a custom one.\n    .parameter(\"custom_param\", json!(true))\n    .build(&amp;registry)?;\n\n// Now you have an LLMProvider instance and can use it.\nprintln!(\"Successfully created provider instance for 'my_openai_plugin'\");\n// e.g., provider.chat(&amp;messages).await?\n\n# Ok(())\n# }\n</code></pre> <p>You can also interact with the registry directly to list available factories or get their configuration schemas.</p> <pre><code># use querymt::plugin::host::PluginRegistry;\n# use querymt::plugin::LLMProviderFactory;\n# use std::sync::Arc;\n# fn example(registry: PluginRegistry) -&gt; anyhow::Result&lt;()&gt; {\n// List all loaded provider factories\nlet all_factories: Vec&lt;Arc&lt;dyn LLMProviderFactory&gt;&gt; = registry.list();\nfor factory in all_factories {\n    println!(\"Loaded provider factory: {}\", factory.name());\n}\n\n// Get a specific provider factory by name\nif let Some(factory) = registry.get(\"my_openai_plugin\") {\n    // Get the config schema for this provider\n    let schema = factory.config_schema();\n    println!(\"Config schema: {}\", serde_json::to_string_pretty(&amp;schema)?);\n}\n# Ok(())\n# }\n</code></pre> <p>This design allows QueryMT to dynamically load and work with multiple LLM providers configured at runtime, without needing to compile them into the main application.</p>"}]}