//! Core execution logic for the agent
//!
//! This module contains `execute_cycle_state_machine`, the state machine
//! used by the kameo `SessionActor` via `ctx.spawn()`.
//!
//! State transitions and tool execution are implemented as free functions below.

mod bridge;
mod llm_retry;
mod maintenance;
mod tool_calls;
mod wait;

use crate::agent::execution_context::ExecutionContext;
use crate::events::{AgentEventKind, ExecutionMetrics, StopType};
use crate::middleware::ExecutionState;
use crate::model::{AgentMessage, MessagePart};
use crate::session::compaction::SessionCompaction;
use agent_client_protocol::StopReason;
use log::{debug, info, trace, warn};
use querymt::chat::ChatRole;
use std::sync::Arc;
use tokio::sync::watch;
use uuid::Uuid;

/// Outcome of a single execution cycle
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CycleOutcome {
    Completed,
    Cancelled,
    Stopped(StopReason),
}

// ══════════════════════════════════════════════════════════════════════════
//  State machine implementation
// ══════════════════════════════════════════════════════════════════════════

use crate::acp::client_bridge::ClientBridgeSender;
use crate::agent::agent_config::AgentConfig;
use crate::agent::core::SnapshotPolicy;
use crate::agent::snapshots::{SnapshotState, snapshot_metadata};
use crate::delegation::{format_delegation_completion_message, format_delegation_failure_message};
use crate::middleware::{
    LlmResponse, ToolCall as MiddlewareToolCall, ToolFunction, ToolResult, WaitCondition,
    WaitReason, calculate_context_tokens,
};
use crate::session::domain::TaskStatus;
use agent_client_protocol::{ContentBlock, ContentChunk, SessionUpdate, TextContent};
use anyhow::Context as _;
use futures_util::StreamExt;
use futures_util::future::join_all;
use querymt::ToolCall;
use querymt::chat::{CacheHint, ChatMessage, FinishReason, StreamChunk};
use querymt::plugin::extism_impl::ExtismChatResponse;

/// Execute the cycle state machine using an `AgentConfig`.
///
/// This is the entry point for the kameo `SessionActor`'s `ctx.spawn()` task.
/// It reads configuration from `AgentConfig` rather than requiring a full agent instance.
///
/// `session_mode` is the per-session `AgentMode` captured at turn start.
/// It is injected into every `ConversationContext` so middleware can read it.
pub(crate) async fn execute_cycle_state_machine(
    config: &AgentConfig,
    exec_ctx: &mut ExecutionContext,
    mut cancel_rx: watch::Receiver<bool>,
    bridge: Option<ClientBridgeSender>,
    session_mode: crate::agent::core::AgentMode,
) -> Result<CycleOutcome, anyhow::Error> {
    debug!(
        "Starting state machine execution (free fn) for session: {}",
        exec_ctx.session_id
    );

    let driver = config.create_driver();

    info!(
        "Session {}: state machine loading history, cancel_rx={}",
        exec_ctx.session_id,
        *cancel_rx.borrow()
    );

    let messages: Arc<[querymt::chat::ChatMessage]> =
        Arc::from(exec_ctx.session_handle.history().await.into_boxed_slice());

    info!(
        "Session {}: history loaded, {} messages",
        exec_ctx.session_id,
        messages.len()
    );
    let turns = messages
        .iter()
        .filter(|msg| matches!(msg.role, ChatRole::User))
        .filter(|msg| matches!(msg.message_type, querymt::chat::MessageType::Text))
        .count();
    let stats = crate::middleware::AgentStats {
        turns,
        ..Default::default()
    };
    let stats = Arc::new(stats);

    let llm_config = exec_ctx
        .llm_config()
        .ok_or_else(|| anyhow::anyhow!("No LLM config for session"))?;

    info!(
        "Session {}: llm_config provider={} model={}",
        exec_ctx.session_id, llm_config.provider, llm_config.model
    );

    let initial_context = Arc::new(
        crate::middleware::ConversationContext::new(
            exec_ctx.session_id.as_str().into(),
            messages,
            stats,
            llm_config.provider.as_str().into(),
            llm_config.model.as_str().into(),
        )
        .with_session_mode(session_mode),
    );

    let mut state = ExecutionState::BeforeLlmCall {
        context: initial_context,
    };
    let mut event_rx = config.event_bus.subscribe();

    state = driver
        .run_turn_start(state, Some(&exec_ctx.runtime))
        .await
        .map_err(|e| anyhow::anyhow!("Middleware error: {}", e))?;

    info!(
        "Session {}: run_turn_start done, state={}, cancel_rx={}",
        exec_ctx.session_id,
        state.name(),
        *cancel_rx.borrow()
    );

    loop {
        if *cancel_rx.borrow() {
            info!(
                "Session {}: CANCELLED at loop top (cancel_rx=true)",
                exec_ctx.session_id
            );
            return Ok(CycleOutcome::Cancelled);
        }

        let state_name = state.name();
        trace!(
            "State machine iteration: {} for session {}",
            state_name, exec_ctx.session_id
        );

        state = match state {
            ExecutionState::BeforeLlmCall { .. } => {
                let state = driver
                    .run_step_start(state, Some(&exec_ctx.runtime))
                    .await
                    .map_err(|e| anyhow::anyhow!("Middleware error: {}", e))?;

                match state {
                    ExecutionState::BeforeLlmCall {
                        context: ref conv_context,
                    } => {
                        transition_before_llm_call(config, conv_context, exec_ctx, &cancel_rx)
                            .await?
                    }
                    other => other,
                }
            }

            ExecutionState::CallLlm {
                ref context,
                ref tools,
            } => transition_call_llm(config, context, tools, &cancel_rx, exec_ctx).await?,

            ExecutionState::AfterLlm { .. } => {
                let state = driver
                    .run_after_llm(state, Some(&exec_ctx.runtime))
                    .await
                    .map_err(|e| anyhow::anyhow!("Middleware error: {}", e))?;
                match state {
                    ExecutionState::AfterLlm {
                        ref response,
                        ref context,
                    } => {
                        transition_after_llm(
                            config,
                            response,
                            context,
                            exec_ctx,
                            &cancel_rx,
                            bridge.as_ref(),
                        )
                        .await?
                    }
                    other => other,
                }
            }

            ExecutionState::ProcessingToolCalls { .. } => {
                let state = driver
                    .run_processing_tool_calls(state, Some(&exec_ctx.runtime))
                    .await
                    .map_err(|e| anyhow::anyhow!("Middleware error: {}", e))?;
                match state {
                    ExecutionState::ProcessingToolCalls {
                        ref remaining_calls,
                        ref results,
                        ref context,
                    } => {
                        transition_processing_tool_calls(
                            config,
                            remaining_calls,
                            results,
                            context,
                            exec_ctx,
                            &cancel_rx,
                            bridge.as_ref(),
                        )
                        .await?
                    }
                    other => other,
                }
            }

            ExecutionState::WaitingForEvent {
                ref context,
                ref wait,
            } => {
                wait::transition_waiting_for_event(
                    config,
                    wait,
                    context,
                    exec_ctx,
                    &mut cancel_rx,
                    &mut event_rx,
                )
                .await?
            }

            ExecutionState::Complete => {
                let turn_end_state = driver
                    .run_turn_end(ExecutionState::Complete, Some(&exec_ctx.runtime))
                    .await
                    .map_err(|e| anyhow::anyhow!("Middleware error: {}", e))?;

                match turn_end_state {
                    ExecutionState::BeforeLlmCall { .. } => turn_end_state,
                    ExecutionState::Complete => {
                        debug!("State machine reached Complete state");

                        if config.pruning_config.enabled
                            && let Err(e) = maintenance::run_pruning(config, exec_ctx).await
                        {
                            warn!("Pruning failed: {}", e);
                        }

                        return Ok(CycleOutcome::Completed);
                    }
                    ExecutionState::Stopped {
                        ref message,
                        stop_type,
                        ..
                    } => {
                        info!("Turn-end middleware stopped: {} ({:?})", message, stop_type);
                        turn_end_state
                    }
                    other => other,
                }
            }

            ExecutionState::Stopped {
                ref message,
                stop_type,
                ..
            } => {
                info!("State machine stopped: {} ({:?})", message, stop_type);

                if stop_type == StopType::ContextThreshold && config.compaction_config.auto {
                    info!("Context threshold reached, triggering AI compaction");

                    match maintenance::run_ai_compaction(config, exec_ctx, &state).await {
                        Ok(new_state) => {
                            state = new_state;
                            continue;
                        }
                        Err(e) => {
                            warn!("AI compaction failed: {}", e);
                        }
                    }
                }

                let metrics = state
                    .context()
                    .map(|ctx| ExecutionMetrics {
                        steps: ctx.stats.steps,
                        turns: ctx.stats.turns,
                    })
                    .unwrap_or_default();

                config.emit_event(
                    &exec_ctx.session_id,
                    AgentEventKind::MiddlewareStopped {
                        stop_type,
                        reason: message.to_string(),
                        metrics,
                    },
                );

                return Ok(CycleOutcome::Stopped(StopReason::from(stop_type)));
            }

            ExecutionState::Cancelled => {
                debug!("State machine cancelled");
                return Ok(CycleOutcome::Cancelled);
            }
        };
    }
}

// ── Transition: BeforeLlmCall ─────────────────────────────────────────────

async fn transition_before_llm_call(
    config: &AgentConfig,
    context: &Arc<crate::middleware::ConversationContext>,
    exec_ctx: &ExecutionContext,
    cancel_rx: &watch::Receiver<bool>,
) -> Result<ExecutionState, anyhow::Error> {
    debug!(
        "BeforeLlmCall: session={}, steps={}",
        exec_ctx.session_id, context.stats.steps
    );

    if *cancel_rx.borrow() {
        return Ok(ExecutionState::Cancelled);
    }

    let provider = exec_ctx
        .session_handle
        .provider()
        .await
        .map_err(|e| anyhow::anyhow!("Failed to build provider: {}", e))?;

    let tools = config.collect_tools(provider, Some(exec_ctx.runtime.as_ref()));

    let tools_json =
        serde_json::to_vec(&tools).context("Failed to serialize tools for hash computation")?;
    let new_hash = crate::hash::RapidHash::new(&tools_json);

    let mut current = exec_ctx.runtime.current_tools_hash.lock().unwrap();
    let changed = current.is_none_or(|h| h != new_hash);
    if changed {
        *current = Some(new_hash);
    }

    if changed {
        config.emit_event(
            &exec_ctx.session_id,
            crate::events::AgentEventKind::ToolsAvailable {
                tools: tools.clone(),
                tools_hash: new_hash,
            },
        );
    }

    Ok(ExecutionState::CallLlm {
        context: context.clone(),
        tools: Arc::from(tools.into_boxed_slice()),
    })
}

// ── Transition: CallLlm ──────────────────────────────────────────────────

/// Apply cache breakpoints to the last 2 messages in the conversation.
fn apply_cache_breakpoints(messages: &[ChatMessage]) -> Vec<ChatMessage> {
    let len = messages.len();
    messages
        .iter()
        .enumerate()
        .map(|(i, msg)| {
            let mut m = msg.clone();
            if len >= 2 && i >= len - 2 {
                m.cache = Some(CacheHint::Ephemeral { ttl_seconds: None });
            }
            m
        })
        .collect()
}

async fn transition_call_llm(
    config: &AgentConfig,
    context: &Arc<crate::middleware::ConversationContext>,
    tools: &Arc<[querymt::chat::Tool]>,
    cancel_rx: &watch::Receiver<bool>,
    exec_ctx: &ExecutionContext,
) -> Result<ExecutionState, anyhow::Error> {
    let session_id = &exec_ctx.session_id;
    debug!(
        "CallLlm: session={}, messages={}",
        session_id,
        context.messages.len()
    );

    if *cancel_rx.borrow() {
        return Ok(ExecutionState::Cancelled);
    }

    config.emit_event(
        session_id,
        AgentEventKind::LlmRequestStart {
            message_count: context.messages.len(),
        },
    );

    let session_handle = &exec_ctx.session_handle;
    let messages_with_cache = apply_cache_breakpoints(&context.messages);

    let response = if tools.is_empty() {
        let cancel_rx_clone = cancel_rx.clone();
        llm_retry::call_llm_with_retry(config, session_id, cancel_rx, || {
            let messages_with_cache = &messages_with_cache;
            let mut cancel_rx_clone = cancel_rx_clone.clone();
            async move {
                tokio::select! {
                    result = session_handle.submit_request(messages_with_cache) => {
                        result.map_err(|e| anyhow::anyhow!("LLM request failed: {}", e))
                    }
                    _ = cancel_rx_clone.changed() => {
                        Err(anyhow::anyhow!("Cancelled"))
                    }
                }
            }
        })
        .await?
    } else {
        let provider = session_handle
            .provider()
            .await
            .map_err(|e| anyhow::anyhow!("Failed to build provider: {}", e))?;

        if context.provider.as_ref() == "codex" {
            let mut stream = provider
                .chat_stream_with_tools(&messages_with_cache, Some(tools))
                .await
                .map_err(|e| anyhow::anyhow!("LLM streaming request with tools failed: {}", e))?;

            let mut text = String::new();
            let mut tool_calls: Vec<ToolCall> = Vec::new();
            let mut tool_call_ids = std::collections::HashSet::new();
            let mut usage: Option<querymt::Usage> = None;

            while let Some(item) = stream.next().await {
                if *cancel_rx.borrow() {
                    return Ok(ExecutionState::Cancelled);
                }

                match item.map_err(|e| {
                    anyhow::anyhow!("LLM streaming request with tools failed: {}", e)
                })? {
                    StreamChunk::Text(delta) => text.push_str(&delta),
                    StreamChunk::Thinking(_) => {}
                    StreamChunk::ToolUseComplete { tool_call, .. } => {
                        if tool_call_ids.insert(tool_call.id.clone()) {
                            tool_calls.push(tool_call);
                        }
                    }
                    StreamChunk::Usage(u) => usage = Some(u),
                    StreamChunk::Done { .. } => break,
                    _ => {}
                }
            }

            let finish_reason = if tool_calls.is_empty() {
                Some(FinishReason::Stop)
            } else {
                Some(FinishReason::ToolCalls)
            };

            Box::new(ExtismChatResponse {
                text: if text.is_empty() { None } else { Some(text) },
                tool_calls: if tool_calls.is_empty() {
                    None
                } else {
                    Some(tool_calls)
                },
                thinking: None,
                usage,
                finish_reason,
            })
        } else {
            let cancel_rx_clone = cancel_rx.clone();
            llm_retry::call_llm_with_retry(config, session_id, cancel_rx, || {
                let provider = &provider;
                let messages_with_cache = &messages_with_cache;
                let tools = tools.as_ref();
                let mut cancel_rx_clone = cancel_rx_clone.clone();
                async move {
                    tokio::select! {
                        result = provider.chat_with_tools(messages_with_cache, Some(tools)) => {
                            result.map_err(|e| anyhow::anyhow!("LLM request with tools failed: {}", e))
                        }
                        _ = cancel_rx_clone.changed() => {
                            Err(anyhow::anyhow!("Cancelled"))
                        }
                    }
                }
            }).await?
        }
    };

    let usage = response.usage();
    let response_content = response.text().unwrap_or_default();
    let tool_calls = response.tool_calls().unwrap_or_default();
    let finish_reason = response.finish_reason();

    let (request_cost, cumulative_cost) = if let Some(usage_info) = response.usage() {
        let pricing = session_handle.get_pricing();
        let request_cost = pricing.as_ref().and_then(|p| {
            p.calculate_cost(
                usage_info.input_tokens as u64,
                usage_info.output_tokens as u64,
            )
        });
        let cumulative_cost = pricing.as_ref().and_then(|p| {
            p.calculate_cost(
                context.stats.total_input_tokens + usage_info.input_tokens as u64,
                context.stats.total_output_tokens + usage_info.output_tokens as u64,
            )
        });
        (request_cost, cumulative_cost)
    } else {
        (None, None)
    };

    info!(
        "Session {} received provider response ({} chars, {} tool call(s), finish: {:?}, cost: ${:.4?})",
        session_id,
        response_content.len(),
        tool_calls.len(),
        finish_reason,
        request_cost,
    );

    let context_tokens = calculate_context_tokens(response.usage().as_ref());

    config.emit_event(
        session_id,
        AgentEventKind::LlmRequestEnd {
            usage: response.usage(),
            tool_calls: tool_calls.len(),
            finish_reason,
            cost_usd: request_cost,
            cumulative_cost_usd: cumulative_cost,
            context_tokens,
            metrics: ExecutionMetrics {
                steps: context.stats.steps + 1,
                turns: context.stats.turns,
            },
        },
    );

    let llm_tool_calls: Vec<MiddlewareToolCall> = tool_calls
        .into_iter()
        .map(|tc| MiddlewareToolCall {
            id: tc.id.clone(),
            function: ToolFunction {
                name: tc.function.name.clone(),
                arguments: tc.function.arguments.clone(),
            },
        })
        .collect();

    Ok(ExecutionState::AfterLlm {
        response: Arc::new(LlmResponse::new(
            response_content,
            llm_tool_calls,
            usage,
            finish_reason,
        )),
        context: context.clone(),
    })
}

// ── Transition: AfterLlm ─────────────────────────────────────────────────

async fn transition_after_llm(
    config: &AgentConfig,
    response: &Arc<LlmResponse>,
    context: &Arc<crate::middleware::ConversationContext>,
    exec_ctx: &mut ExecutionContext,
    cancel_rx: &watch::Receiver<bool>,
    bridge: Option<&ClientBridgeSender>,
) -> Result<ExecutionState, anyhow::Error> {
    debug!(
        "AfterLlm: session={}, has_tool_calls={}",
        exec_ctx.session_id,
        response.has_tool_calls()
    );

    if *cancel_rx.borrow() {
        return Ok(ExecutionState::Cancelled);
    }

    let progress_description = if response.has_tool_calls() {
        format!(
            "Received response with {} tool call(s)",
            response.tool_calls.len()
        )
    } else {
        "Received response from LLM".to_string()
    };

    let progress_entry = exec_ctx
        .state
        .record_progress(
            crate::session::domain::ProgressKind::Note,
            progress_description,
            None,
        )
        .await
        .map_err(|e| anyhow::anyhow!("Failed to record progress: {}", e))?;

    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::ProgressRecorded { progress_entry },
    );

    let mut parts = Vec::new();
    if !response.content.is_empty() {
        bridge::send_session_update(
            bridge,
            &exec_ctx.session_id,
            SessionUpdate::AgentMessageChunk(ContentChunk::new(ContentBlock::Text(
                TextContent::new(response.content.clone()),
            ))),
        );
        parts.push(MessagePart::Text {
            content: response.content.clone(),
        });
    }

    for call in &response.tool_calls {
        parts.push(MessagePart::ToolUse(querymt::ToolCall {
            id: call.id.clone(),
            call_type: "function".to_string(),
            function: querymt::FunctionCall {
                name: call.function.name.clone(),
                arguments: call.function.arguments.clone(),
            },
        }));
    }

    let assistant_msg = AgentMessage {
        id: Uuid::new_v4().to_string(),
        session_id: exec_ctx.session_id.clone(),
        role: ChatRole::Assistant,
        parts,
        created_at: time::OffsetDateTime::now_utc().unix_timestamp(),
        parent_message_id: None,
    };

    exec_ctx
        .add_message(assistant_msg.clone())
        .await
        .map_err(|e| anyhow::anyhow!("Failed to store assistant message: {}", e))?;

    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::AssistantMessageStored {
            content: response.content.clone(),
            message_id: Some(assistant_msg.id.clone()),
        },
    );

    let mut messages = (*context.messages).to_vec();
    messages.push(assistant_msg.to_chat_message());

    let mut updated_stats = (*context.stats).clone();
    if let Some(token_usage) = &response.usage {
        updated_stats.total_input_tokens += token_usage.input_tokens as u64;
        updated_stats.total_output_tokens += token_usage.output_tokens as u64;
        updated_stats.reasoning_tokens += token_usage.reasoning_tokens as u64;
        updated_stats.cache_read_tokens += token_usage.cache_read as u64;
        updated_stats.cache_write_tokens += token_usage.cache_write as u64;
        updated_stats.context_tokens = calculate_context_tokens(Some(token_usage)) as usize;
        updated_stats.steps += 1;

        if let Some(pricing) = exec_ctx.session_handle.get_pricing() {
            updated_stats.update_costs(&pricing);
        }
    }

    let new_context = Arc::new(
        crate::middleware::ConversationContext::new(
            context.session_id.clone(),
            Arc::from(messages.into_boxed_slice()),
            Arc::new(updated_stats),
            context.provider.clone(),
            context.model.clone(),
        )
        .with_session_mode(context.session_mode),
    );

    match response.finish_reason {
        Some(FinishReason::ToolCalls) => {
            if !response.tool_calls.is_empty() {
                Ok(ExecutionState::ProcessingToolCalls {
                    remaining_calls: Arc::from(response.tool_calls.clone().into_boxed_slice()),
                    results: Arc::from(Vec::new().into_boxed_slice()),
                    context: new_context,
                })
            } else {
                Ok(ExecutionState::Complete)
            }
        }

        Some(FinishReason::Stop) => {
            if exec_ctx.state.active_task.is_some() {
                if let Err(e) = exec_ctx.state.update_task_status(TaskStatus::Done).await {
                    debug!("Failed to auto-complete task on stop: {}", e);
                } else if let Some(task) = exec_ctx.state.active_task.clone() {
                    config.emit_event(
                        &exec_ctx.session_id,
                        AgentEventKind::TaskStatusChanged { task },
                    );
                }
            }
            Ok(ExecutionState::Complete)
        }

        Some(FinishReason::Length) => Ok(ExecutionState::Stopped {
            message: "Model hit token limit".into(),
            stop_type: StopType::ModelTokenLimit,
            context: Some(new_context),
        }),

        Some(FinishReason::ContentFilter) => Ok(ExecutionState::Stopped {
            message: "Response blocked by content filter".into(),
            stop_type: StopType::ContentFilter,
            context: Some(new_context),
        }),

        Some(FinishReason::Error)
        | Some(FinishReason::Other)
        | Some(FinishReason::Unknown)
        | None => {
            if response.tool_calls.is_empty() {
                Ok(ExecutionState::Complete)
            } else {
                Ok(ExecutionState::ProcessingToolCalls {
                    remaining_calls: Arc::from(response.tool_calls.clone().into_boxed_slice()),
                    results: Arc::from(Vec::new().into_boxed_slice()),
                    context: new_context,
                })
            }
        }
    }
}

// ── Transition: ProcessingToolCalls ───────────────────────────────────────

async fn transition_processing_tool_calls(
    config: &AgentConfig,
    remaining_calls: &Arc<[MiddlewareToolCall]>,
    results: &Arc<[ToolResult]>,
    context: &Arc<crate::middleware::ConversationContext>,
    exec_ctx: &mut ExecutionContext,
    cancel_rx: &watch::Receiver<bool>,
    bridge: Option<&ClientBridgeSender>,
) -> Result<ExecutionState, anyhow::Error> {
    debug!(
        "ProcessingToolCalls: session={}, remaining={}, completed={}",
        exec_ctx.session_id,
        remaining_calls.len(),
        results.len()
    );

    if *cancel_rx.borrow() {
        return Ok(ExecutionState::Cancelled);
    }

    if remaining_calls.is_empty() {
        return store_all_tool_results(config, results, context, exec_ctx).await;
    }

    debug!(
        "Executing {} tool calls in parallel for session {}",
        remaining_calls.len(),
        exec_ctx.session_id
    );

    let futures: Vec<_> = remaining_calls
        .iter()
        .map(|call| execute_tool_call(config, call, exec_ctx, bridge))
        .collect();

    let mut cancel_rx_clone = cancel_rx.clone();
    let tool_results = tokio::select! {
        results = join_all(futures) => results,
        _ = cancel_rx_clone.changed() => {
            return Ok(ExecutionState::Cancelled);
        }
    };

    let mut all_results = (**results).to_vec();
    for result in tool_results {
        all_results.push(result?);
    }

    debug!(
        "Completed {} tool calls for session {}",
        all_results.len() - results.len(),
        exec_ctx.session_id
    );

    Ok(ExecutionState::ProcessingToolCalls {
        remaining_calls: Arc::from(Vec::<MiddlewareToolCall>::new().into_boxed_slice()),
        results: Arc::from(all_results.into_boxed_slice()),
        context: context.clone(),
    })
}

// ── Tool execution moved to tool_calls.rs ────────────────────────────────

// Placeholder for removed execute_tool_call function - see tool_calls.rs
/*
async fn execute_tool_call(
    config: &AgentConfig,
    call: &MiddlewareToolCall,
    exec_ctx: &ExecutionContext,
    bridge: Option<&ClientBridgeSender>,
) -> Result<ToolResult, anyhow::Error> {
    debug!(
        "Executing tool: session={}, tool={}",
        exec_ctx.session_id, call.function.name
    );

    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::ToolCallStart {
            tool_call_id: call.id.clone(),
            tool_name: call.function.name.clone(),
            arguments: call.function.arguments.clone(),
        },
    );

    let snapshot = if config.should_snapshot_tool(&call.function.name) {
        config
            .prepare_snapshot(exec_ctx.cwd())
            .map(|(root, policy)| {
                config.emit_event(
                    &exec_ctx.session_id,
                    AgentEventKind::SnapshotStart {
                        policy: policy.to_string(),
                    },
                );
                match policy {
                    SnapshotPolicy::Diff => {
                        let pre_tree = crate::index::merkle::MerkleTree::scan(root.as_path());
                        SnapshotState::Diff { pre_tree, root }
                    }
                    SnapshotPolicy::Metadata => SnapshotState::Metadata { root },
                    SnapshotPolicy::None => SnapshotState::None,
                }
            })
            .unwrap_or(SnapshotState::None)
    } else {
        SnapshotState::None
    };

    let progress_entry = exec_ctx
        .state
        .record_progress(
            crate::session::domain::ProgressKind::ToolCall,
            format!("Calling tool: {}", call.function.name),
            Some(serde_json::from_str(&call.function.arguments).unwrap_or_default()),
        )
        .await
        .map_err(|e| anyhow::anyhow!("Failed to record progress: {}", e))?;

    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::ProgressRecorded { progress_entry },
    );

    let args: serde_json::Value =
        serde_json::from_str(&call.function.arguments).unwrap_or_else(|_| serde_json::json!({}));

    // Set up elicitation channel for this tool call
    let (elicitation_tx, mut elicitation_rx) =
        tokio::sync::mpsc::channel::<crate::tools::ElicitationRequest>(1);

    let event_bus = config.event_bus.clone();
    let session_id_clone = exec_ctx.session_id.clone();
    let pending_elicitations = config.pending_elicitations.clone();
    tokio::spawn(async move {
        while let Some(request) = elicitation_rx.recv().await {
            let elicitation_id = request.elicitation_id.clone();
            {
                let mut pending = pending_elicitations.lock().await;
                pending.insert(elicitation_id.clone(), request.response_tx);
            }
            event_bus.publish(
                &session_id_clone,
                crate::events::AgentEventKind::ElicitationRequested {
                    elicitation_id,
                    session_id: session_id_clone.clone(),
                    message: request.message,
                    requested_schema: request.requested_schema,
                    source: request.source,
                },
            );
        }
    });

    let tool_context = exec_ctx.tool_context(config.agent_registry.clone(), Some(elicitation_tx));

    let (raw_result_json, is_error) = if !config.is_tool_allowed(&call.function.name) {
        (
            format!("Error: tool '{}' is not allowed", call.function.name),
            true,
        )
    } else if let Some(tool) = config.tool_registry.find(&call.function.name) {
        match tool.call(args.clone(), &tool_context).await {
            Ok(res) => (res, false),
            Err(e) => (format!("Error: {}", e), true),
        }
    } else if let Some(tool) = exec_ctx.runtime.mcp_tools.get(&call.function.name) {
        use querymt::tool_decorator::CallFunctionTool;
        match tool.call(args.clone()).await {
            Ok(res) => (res, false),
            Err(e) => (format!("Error: {}", e), true),
        }
    } else if !ensure_tool_permission(
        config,
        exec_ctx,
        &call.id,
        &call.function.name,
        &args,
        bridge,
    )
    .await
    .map_err(|e| anyhow::anyhow!("Permission check failed: {}", e))?
    {
        ("Error: permission denied".to_string(), true)
    } else {
        match exec_ctx
            .session_handle
            .call_tool(&call.function.name, args.clone())
            .await
        {
            Ok(res) => (res, false),
            Err(e) => (format!("Error: {}", e), true),
        }
    };

    // Apply Layer 1 truncation
    let result_json = if !is_error {
        use crate::tools::builtins::helpers::{
            TruncationDirection, format_truncation_message_with_overflow, save_overflow_output,
            truncate_output,
        };
        let tc = &config.tool_output_config;
        let truncation = truncate_output(
            &raw_result_json,
            tc.max_lines,
            tc.max_bytes,
            TruncationDirection::Head,
        );
        if truncation.was_truncated {
            let overflow = save_overflow_output(
                &raw_result_json,
                &tc.overflow_storage,
                &exec_ctx.session_id,
                &call.id,
                None,
            );

            let tool_hint = config
                .tool_registry
                .find(&call.function.name)
                .and_then(|t| t.truncation_hint());

            let suffix = format_truncation_message_with_overflow(
                &truncation,
                TruncationDirection::Head,
                Some(&overflow),
                tool_hint,
            );
            format!("{}{}", truncation.content, suffix)
        } else {
            raw_result_json
        }
    } else {
        raw_result_json
    };

    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::ToolCallEnd {
            tool_call_id: call.id.clone(),
            tool_name: call.function.name.clone(),
            is_error,
            result: result_json.clone(),
        },
    );

    let snapshot_part = match snapshot {
        SnapshotState::Diff { pre_tree, root } => {
            let post_tree = crate::index::merkle::MerkleTree::scan(root.as_path());
            let changed_paths = post_tree.diff_paths(&pre_tree);
            config.emit_event(
                &exec_ctx.session_id,
                AgentEventKind::SnapshotEnd {
                    summary: Some(changed_paths.summary()),
                },
            );
            Some(MessagePart::Snapshot {
                root_hash: post_tree.root_hash,
                changed_paths,
            })
        }
        SnapshotState::Metadata { root } => {
            let (part, summary) = snapshot_metadata(root.as_path());
            config.emit_event(
                &exec_ctx.session_id,
                AgentEventKind::SnapshotEnd { summary },
            );
            Some(part)
        }
        SnapshotState::None => None,
    };

    let mut tool_result = ToolResult::new(
        call.id.clone(),
        result_json,
        is_error,
        Some(call.function.name.clone()),
        Some(call.function.arguments.clone()),
    );
    if let Some(part) = snapshot_part {
        tool_result = tool_result.with_snapshot(part);
    }

    Ok(tool_result)
}

// ── Permission checking (free function) ──────────────────────────────────

async fn ensure_tool_permission(
    config: &AgentConfig,
    exec_ctx: &ExecutionContext,
    tool_call_id: &str,
    tool_name: &str,
    args: &serde_json::Value,
    bridge: Option<&ClientBridgeSender>,
) -> Result<bool, agent_client_protocol::Error> {
    use crate::agent::utils::{extract_locations, tool_kind_for_tool};
    use agent_client_protocol::{
        PermissionOption, PermissionOptionId, PermissionOptionKind, RequestPermissionOutcome,
        RequestPermissionRequest, ToolCallId, ToolCallStatus, ToolCallUpdate, ToolCallUpdateFields,
    };

    let requires_permission = config.requires_permission_for_tool(tool_name);
    if !requires_permission {
        return Ok(true);
    }

    if let Some(task) = &exec_ctx.state.active_task
        && task.status != TaskStatus::Active
    {
        return Ok(false);
    }

    if let Ok(cache) = exec_ctx.runtime.permission_cache.lock()
        && let Some(cached) = cache.get(tool_name)
    {
        return Ok(*cached);
    }

    let permission_id = Uuid::new_v4().to_string();
    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::PermissionRequested {
            permission_id: permission_id.clone(),
            task_id: exec_ctx
                .state
                .active_task
                .as_ref()
                .map(|task| task.public_id.clone()),
            tool_name: tool_name.to_string(),
            reason: format!("Tool {} requires explicit permission", tool_name),
        },
    );

    // In the actor model, only the bridge is available (no client)
    let Some(bridge) = bridge else {
        // No bridge available — auto-grant permission
        config.emit_event(
            &exec_ctx.session_id,
            AgentEventKind::PermissionGranted {
                permission_id,
                granted: true,
            },
        );
        return Ok(true);
    };

    let locations = extract_locations(args);
    let tool_update_fields = ToolCallUpdateFields::new()
        .title(format!("Run {}", tool_name))
        .kind(tool_kind_for_tool(tool_name))
        .status(ToolCallStatus::Pending)
        .locations(if locations.is_empty() {
            None
        } else {
            Some(locations)
        })
        .raw_input(args.clone());

    let request = RequestPermissionRequest::new(
        exec_ctx.session_id.clone(),
        ToolCallUpdate::new(
            ToolCallId::from(tool_call_id.to_string()),
            tool_update_fields,
        ),
        vec![
            PermissionOption::new(
                PermissionOptionId::from("allow_once"),
                "Allow once",
                PermissionOptionKind::AllowOnce,
            ),
            PermissionOption::new(
                PermissionOptionId::from("allow_always"),
                "Always allow",
                PermissionOptionKind::AllowAlways,
            ),
            PermissionOption::new(
                PermissionOptionId::from("reject_once"),
                "Reject once",
                PermissionOptionKind::RejectOnce,
            ),
            PermissionOption::new(
                PermissionOptionId::from("reject_always"),
                "Always reject",
                PermissionOptionKind::RejectAlways,
            ),
        ],
    );

    let response = bridge.request_permission(request).await?;
    let granted = match response.outcome {
        RequestPermissionOutcome::Selected(selected) => {
            let option_id = selected.option_id.0.as_ref();
            let allow = option_id == "allow_once" || option_id == "allow_always";
            if let Ok(mut cache) = exec_ctx.runtime.permission_cache.lock() {
                if option_id == "allow_always" {
                    cache.insert(tool_name.to_string(), true);
                } else if option_id == "reject_always" {
                    cache.insert(tool_name.to_string(), false);
                }
            }
            allow
        }
        _ => false,
    };

    config.emit_event(
        &exec_ctx.session_id,
        AgentEventKind::PermissionGranted {
            permission_id,
            granted,
        },
    );

    Ok(granted)
}

// ── Store tool results (free function) ───────────────────────────────────

async fn record_tool_side_effects(
    config: &AgentConfig,
    result: &ToolResult,
    exec_ctx: &ExecutionContext,
) -> Option<WaitCondition> {
    if result.is_error {
        return None;
    }

    let tool_name = result.tool_name.as_ref()?;

    if tool_name == "write_file" || tool_name == "apply_patch" {
        let args: serde_json::Value =
            serde_json::from_str(result.tool_arguments.as_deref().unwrap_or("{}"))
                .unwrap_or_default();
        let path = args
            .get("path")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());

        if let Ok(artifact) = exec_ctx
            .state
            .record_artifact(
                "file".to_string(),
                None,
                path.clone(),
                Some(format!("Produced by {}", tool_name)),
            )
            .await
        {
            config.emit_event(
                &exec_ctx.session_id,
                AgentEventKind::ArtifactRecorded { artifact },
            );
        }
    }

    if tool_name == "delegate" {
        let args: serde_json::Value =
            serde_json::from_str(result.tool_arguments.as_deref().unwrap_or("{}"))
                .unwrap_or_default();
        let target_agent_id = args
            .get("target_agent_id")
            .and_then(|v| v.as_str())
            .unwrap_or_default()
            .to_string();
        let objective = args
            .get("objective")
            .and_then(|v| v.as_str())
            .unwrap_or_default()
            .to_string();
        let context_val = args
            .get("context")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());
        let constraints = args
            .get("constraints")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());
        let expected_output = args
            .get("expected_output")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());

        if let Ok(delegation) = exec_ctx
            .state
            .record_delegation(
                target_agent_id.clone(),
                objective.clone(),
                context_val.clone(),
                constraints,
                expected_output,
            )
            .await
        {
            config.emit_event(
                &exec_ctx.session_id,
                AgentEventKind::DelegationRequested {
                    delegation: delegation.clone(),
                },
            );
            return Some(WaitCondition::delegation(delegation.public_id.clone()));
        }
    }

    None
}

async fn store_all_tool_results(
    config: &AgentConfig,
    results: &Arc<[ToolResult]>,
    context: &Arc<crate::middleware::ConversationContext>,
    exec_ctx: &mut ExecutionContext,
) -> Result<ExecutionState, anyhow::Error> {
    debug!(
        "Storing all tool results: session={}, count={}",
        exec_ctx.session_id,
        results.len()
    );

    let mut messages = (*context.messages).to_vec();
    let mut wait_conditions = Vec::new();

    for result in results.iter() {
        let mut parts = vec![MessagePart::ToolResult {
            call_id: result.call_id.clone(),
            content: result.content.clone(),
            is_error: result.is_error,
            tool_name: result.tool_name.clone(),
            tool_arguments: result.tool_arguments.clone(),
            compacted_at: None,
        }];
        if let Some(ref snapshot) = result.snapshot_part {
            parts.push(snapshot.clone());
        }

        let result_msg = AgentMessage {
            id: Uuid::new_v4().to_string(),
            session_id: exec_ctx.session_id.clone(),
            role: ChatRole::User,
            parts,
            created_at: time::OffsetDateTime::now_utc().unix_timestamp(),
            parent_message_id: None,
        };

        exec_ctx
            .add_message(result_msg.clone())
            .await
            .map_err(|e| anyhow::anyhow!("Failed to store tool result: {}", e))?;

        messages.push(result_msg.to_chat_message());

        if let Some(wait_condition) = record_tool_side_effects(config, result, exec_ctx).await
        {
            wait_conditions.push(wait_condition);
        }
    }

    let new_context = Arc::new(
        crate::middleware::ConversationContext::new(
            context.session_id.clone(),
            Arc::from(messages.into_boxed_slice()),
            context.stats.clone(),
            context.provider.clone(),
            context.model.clone(),
        )
        .with_session_mode(context.session_mode),
    );

    // Aggregate changed file paths from tool results for dedup check
    let mut combined = crate::index::DiffPaths::default();
    for result in results.iter() {
        if let Some(ref snapshot) = result.snapshot_part
            && let Some(paths) = snapshot.changed_paths()
        {
            combined.added.extend(paths.added.iter().cloned());
            combined.modified.extend(paths.modified.iter().cloned());
            combined.removed.extend(paths.removed.iter().cloned());
        }
    }

    combined.added.sort();
    combined.added.dedup();
    combined.modified.sort();
    combined.modified.dedup();
    combined.removed.sort();
    combined.removed.dedup();

    if !combined.is_empty()
        && let Ok(mut diffs) = exec_ctx.runtime.turn_diffs.lock()
    {
        diffs.added.extend(combined.added);
        diffs.modified.extend(combined.modified);
        diffs.removed.extend(combined.removed);
        diffs.added.sort();
        diffs.added.dedup();
        diffs.modified.sort();
        diffs.modified.dedup();
        diffs.removed.sort();
        diffs.removed.dedup();
    }

    if let Some(wait_condition) = WaitCondition::merge(wait_conditions) {
        return Ok(ExecutionState::WaitingForEvent {
            context: new_context,
            wait: wait_condition,
        });
    }

    Ok(ExecutionState::BeforeLlmCall {
        context: new_context,
    })
}

// ── Transition: WaitingForEvent moved to wait.rs ─────────────────────────

// ── LLM retry logic moved to llm_retry.rs ────────────────────────────────

// ── Pruning and AI compaction moved to maintenance.rs ────────────────────

// ── Session update helper moved to bridge.rs ─────────────────────────────
